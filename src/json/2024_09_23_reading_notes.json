{
    "1": {
        "name": "Baoyang Chen",
        "content": "<p><a href=\"https://arxiv.org/pdf/2407.20229\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https://arxiv.org/pdf/2407.20229</a></p>\n<p>Not quite I expected to read on a bump flight:</p>\n<p>The paper\u2019s approach to lifting 2D features into 3D representations and using these for fine-tuning could have notable implications for creating contents, particularly in the areas of virtual and augmented reality (VR/AR), video games, and other multimedia applications where 3D content is essential.</p>\n<ol>\n<li>Enhanced Realism in Synthetic Video Generation: By improving the 3D understanding of scenes from merely 2D data, the technique can help generate more realistic synthetic videos where depth perception and spatial accuracy are crucial. For example, in virtual reality, more accurate 3D scene representations can lead to more immersive experiences as the environments would feel more lifelike and spatially coherent.</li>\n<li>Improved Augmentation in AR Applications: In augmented reality, accurately overlaying virtual objects onto real-world footage requires a good understanding of the scene\u2019s depth and geometry. The paper\u2019s method could enhance the ability of AR systems to place and scale virtual objects appropriately within a real environment, making them interact more naturally with real-world elements.</li>\n<li>Video Game Development: Video game designers could use such enhanced 2D-to-3D conversion techniques to create more dynamic and responsive game environments that can be rendered from various viewpoints, offering a richer and more engaging player experience.</li>\n</ol>\n<p>My further questions:</p>\n<p>Can the 3D-aware fine-tuning adapt to changes over time in a dynamic scene, such as moving objects or varying lighting conditions?</p>\n<p>Considering the fine-tuning and rendering processes, is it feasible to integrate this technology into real-time applications such as live video analysis or augmented reality?</p>\n<p>I want to investigate if such method can be combined with existing pipeline with UE engine.</p>\n<p>I want to know more about this paper:</p>\n<p>While the paper introduces a novel method, the specifics of certain algorithmic choices and parameter settings may not be fully detailed. For instance, how the 3D Gaussian parameters are chosen, the exact nature of the fine-tuning process, and the optimizations used could benefit from a more in-depth explanation to ensure reproducibility and to allow others to build upon this work.</p>\n<p>I\u2019m not sure about this: the method\u2019s effectiveness is mainly shown through qualitative assessments and broad performance metrics on subsequent tasks, a more detailed statistical analysis would be beneficial. Including confidence intervals and error analysis in the measurements could significantly enhance our understanding of the method\u2019s reliability and define its performance limits more clearly. As an artist, I appreciate qualitative analysis, but precise quantitative evaluations can provide additional insights.</p>",
        "update": "2024-9-22 14:19:28"
    },
    "2": {
        "name": "Hanlu Ma",
        "content": "<h1>\n<a name=\"paper-titledisclose-to-tell-a-data-design-framework-for-alternative-narratives-1\" class=\"anchor\" href=\"#paper-titledisclose-to-tell-a-data-design-framework-for-alternative-narratives-1\"></a>[Paper Title]Disclose to Tell: a Data Design Framework for Alternative Narratives</h1>\n<ul>\n<li>Conference/Journal (CSCW-2021):</li>\n<li>Authors/group: Mar\u00eda de los \u00c1ngeles Briones Rojas</li>\n<li>Github link:</li>\n<li>Project page: <a href=\"https://link.springer.com/article/10.1007/s10606-021-09416-1#ref-CR3\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Disclose to Tell: a Data Design Framework for Alternative Narratives | Computer Supported Cooperative Work (CSCW)</a>\n</li>\n</ul>\n<hr>\n<h2>\n<a name=\"spiral_notepad-summary-2\" class=\"anchor\" href=\"#spiral_notepad-summary-2\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/spiral_notepad.png?v=12\" title=\":spiral_notepad:\" class=\"emoji\" alt=\":spiral_notepad:\" loading=\"lazy\" width=\"20\" height=\"20\"> Summary</h2>\n<p>This paper presents a data design framework for visualizations used in alternative narratives. It is introduced as a methodological tool that encourages actionable data practices, promoting a more critical and reflective data culture. The framework is structured in two ways of approaching the process of working with data: from the parts of the process and from the process as a whole. The first approach presents four lenses intended to materialize aspects of the process of working with and making sense of data: Open/close, Composition, Zoom, and Sanitization. The second approach proposes a self-hacking operation of disclosing the production process of the visualizations.</p>\n<hr>\n<h2>\n<a name=\"thinking-critical-thinking-3\" class=\"anchor\" href=\"#thinking-critical-thinking-3\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/thinking.png?v=12\" title=\":thinking:\" class=\"emoji\" alt=\":thinking:\" loading=\"lazy\" width=\"20\" height=\"20\"> Critical Thinking</h2>\n<h4>\n<a name=\"sparkles-positive-aspects-4\" class=\"anchor\" href=\"#sparkles-positive-aspects-4\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/sparkles.png?v=12\" title=\":sparkles:\" class=\"emoji\" alt=\":sparkles:\" loading=\"lazy\" width=\"20\" height=\"20\"> Positive Aspects</h4>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"//bbs.hkustvis.org/uploads/default/original/2X/b/bf73a263a8dca0f1c57026a59f37301d6cef4506.jpeg\" data-download-href=\"//bbs.hkustvis.org/uploads/default/bf73a263a8dca0f1c57026a59f37301d6cef4506\" title=\"image\"><img src=\"//bbs.hkustvis.org/uploads/default/optimized/2X/b/bf73a263a8dca0f1c57026a59f37301d6cef4506_2_554x500.jpeg\" alt=\"image\" data-base62-sha1=\"rjEWq5O5Ze3zXJOlESTKg4ihISG\" width=\"554\" height=\"500\" srcset=\"//bbs.hkustvis.org/uploads/default/optimized/2X/b/bf73a263a8dca0f1c57026a59f37301d6cef4506_2_554x500.jpeg, //bbs.hkustvis.org/uploads/default/optimized/2X/b/bf73a263a8dca0f1c57026a59f37301d6cef4506_2_831x750.jpeg 1.5x, //bbs.hkustvis.org/uploads/default/original/2X/b/bf73a263a8dca0f1c57026a59f37301d6cef4506.jpeg 2x\" data-dominant-color=\"E3E3E3\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">838\u00d7756 93 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<ul>\n<li>In Figure 1, the alluvial diagram represents the quantitative and qualitative flows of how projects use and release data. Compared to the tables commonly seen in the analysis section, this is a more innovative choice. This visual language effectively represents the fluidity of the narrative.</li>\n<li>In section 3.1, the paper first analyzes existing visualization practices and identifies common flaws (inconsistent with the concept of open data). Then, in sections 3.2 and 3.3, the paper describes the new visualization production framework and its value.</li>\n<li>This paper subsequently reorganized these ways of doing based on previous conceptual works and linked to the ethics of data, situating them as material. Through this metaphor, the paper brought the data closer to the direct experience we have with materials that we can touch, strain, and open.</li>\n</ul>\n<h4>\n<a name=\"cloud_with_rain-negative-aspects-5\" class=\"anchor\" href=\"#cloud_with_rain-negative-aspects-5\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/cloud_with_rain.png?v=12\" title=\":cloud_with_rain:\" class=\"emoji\" alt=\":cloud_with_rain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Negative Aspects</h4>\n<ul>\n<li>TBC</li>\n</ul>\n<hr>\n<h2>\n<a name=\"fountain-creative-thinking-6\" class=\"anchor\" href=\"#fountain-creative-thinking-6\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/fountain.png?v=12\" title=\":fountain:\" class=\"emoji\" alt=\":fountain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Creative thinking:</h2>\n<ul>\n<li>Identified some keywords that might be relevant to ongoing practices: Data-driven alternative narratives, Data activism, Adversarial Design, Weaponized Design.</li>\n<li>Besides discussing general data storytelling methodologies, the paper can also focus on data storytelling for specific purposes and the alignment between the methods and the goals of storytelling.</li>\n</ul>",
        "update": "2024-9-22 5:34:3"
    },
    "3": {
        "name": "Runhua ZHANG",
        "content": "<h1>\n<a name=\"data-storytelling-papers-published-by-xian-1\" class=\"anchor\" href=\"#data-storytelling-papers-published-by-xian-1\"></a>Data Storytelling Papers Published by Xian</h1>\n<p>As a year-1 PhD in IIP, I believe the best way to build concepts about our lab\u2019s research methods is to read the papers published by us. Since I am currently working with Xian recently on Storytelling Moon-shot Project, I guess it will be a good starting point from Xian\u2019s papers. This week, I read the two published on CHI 2022, and CHI 2023. I learned more about data video, the research methods used to contribute design space. Most importantly, I realised how smart it was to introduce cinematic knowledge (Xian\u2019s expertise) into data video. <strong>This made me reflect how I can find a way to combine my background and knowledge from other field into visualization or data video.</strong></p>\n<p>In this reading note, I would focus on the paper published at CHI 2023 titled Is It the End? Guidelines for Cinematic Endings in Data Videos, to share the inspirations and my thoughts.</p>\n<h2>\n<a name=\"is-it-the-end-guidelines-for-cinematic-endings-in-data-videos-2\" class=\"anchor\" href=\"#is-it-the-end-guidelines-for-cinematic-endings-in-data-videos-2\"></a>Is It the End? Guidelines for Cinematic Endings in Data Videos</h2>\n<ul>\n<li>TAG from Runhua: Research Contribution Type: Design Space</li>\n<li>Authors/group: VisLab, Xian Xu</li>\n<li>Project page: <a href=\"https://dl.acm.org/doi/10.1145/3544548.3580701\" rel=\"noopener nofollow ugc\">https://dl.acm.org/doi/10.1145/3544548.3580701</a>\n</li>\n</ul>\n<h2>\n<a name=\"from-wow-to-why-guidelines-for-creating-the-opening-of-a-data-video-with-cinematic-styles-3\" class=\"anchor\" href=\"#from-wow-to-why-guidelines-for-creating-the-opening-of-a-data-video-with-cinematic-styles-3\"></a>From \u2018Wow\u2019 to \u2018Why\u2019: Guidelines for Creating the Opening of a Data Video with Cinematic Styles</h2>\n<ul>\n<li>TAG from Runhua: Research Contribution Type: Design Space</li>\n<li>Authors/group: VisLab, Xian Xu</li>\n<li>Project page: <a href=\"https://dl.acm.org/doi/10.1145/3491102.3501896\" rel=\"noopener nofollow ugc\">https://dl.acm.org/doi/10.1145/3491102.3501896</a>\n</li>\n</ul>\n<hr>\n<h2>\n<a name=\"wink-summary-takeaways-for-who-is-reading-this-note-4\" class=\"anchor\" href=\"#wink-summary-takeaways-for-who-is-reading-this-note-4\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/wink.png?v=12\" title=\":wink:\" class=\"emoji\" alt=\":wink:\" loading=\"lazy\" width=\"20\" height=\"20\">  Summary (Takeaways for who is reading this note)</h2>\n<p><strong>Contribution:</strong> Both these two papers contributed to data video domains by creating new knowledge (design guidelines) on how to create attracting opennings or impressive endings by introducing cinematic techniques.</p>\n<p><strong>Research Methods:</strong> Two papers shared the similar research pipeline:</p>\n<ul>\n<li>\n<p><strong>STEP 1: Understanding the cinematic ending/beginning styles by corpus analysis/coding on a large number of successful and famous films (or and data videos)</strong>.\nFour styles of ending were proposed using punctuation marks: (.) full stop; (!) exclamation point;  (?) question mark; and (\u2026) ellipsis. Six styles of openings were proposed: symbolism &amp; metaphor, camera eye, creative sound, big bang, old footage, ending first.</p>\n</li>\n<li>\n<p><strong>STEP 2: Formulating Guidelines by interviewing experts to investigate how the cinematic opening/ending styles can be applied to data video.</strong> 20 guidelines were proposed for endings, and 28 guidelines for openings.</p>\n</li>\n<li>\n<p><strong>STEP 3: Evaluation on the guidelines by inviting participants to create storyboards with/without guidelines, and inviting experts/general audiences to rate the storyboards.</strong> The evaluation typically had two purposes: (1) whether the guidelines are understandable, and (2) if the application of guidelines can improve attractiveness of data video\u2019s openings or the impression/reflection/persuasion of the endings.</p>\n</li>\n</ul>\n<hr>\n<h2>\n<a name=\"thinking-critical-thinking-5\" class=\"anchor\" href=\"#thinking-critical-thinking-5\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/thinking.png?v=12\" title=\":thinking:\" class=\"emoji\" alt=\":thinking:\" loading=\"lazy\" width=\"20\" height=\"20\"> Critical Thinking</h2>\n<h3>\n<a name=\"aspects-can-be-transferred-to-other-research-6\" class=\"anchor\" href=\"#aspects-can-be-transferred-to-other-research-6\"></a>Aspects can be transferred to other research:</h3>\n<ul>\n<li>The idea: introducing other domains knowledge into data video;</li>\n<li>Research pipeline: showing how to transfer the knowledge from other domain into HCI/data video;</li>\n<li>Evaluation methods: using <strong>storyboards</strong> as the participants\u2019 output was smart! (I initially thought that these two research would ask participants to create <strong>videos</strong> for further evaluation)</li>\n</ul>\n<h3>\n<a name=\"something-is-hard-to-be-copied-7\" class=\"anchor\" href=\"#something-is-hard-to-be-copied-7\"></a>Something is hard to be copied</h3>\n<ul>\n<li>The resources in film industry: both papers invited experts from the film industry. This was hard for others who do not have access to these resources to follow the research. (I think this is a resource moat)</li>\n<li>The descriptions on the films\u2019 ending/openings required good writing skills.</li>\n</ul>\n<hr>\n<h2>\n<a name=\"fountain-creative-thinking-8\" class=\"anchor\" href=\"#fountain-creative-thinking-8\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/fountain.png?v=12\" title=\":fountain:\" class=\"emoji\" alt=\":fountain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Creative thinking:</h2>\n<ul>\n<li>F2F, talking to Xian about <strong>how they achieved agreements on the Step 1 among the co-authors</strong>. Everyone may have different interpretations on the opening/ending styles, right? (a thousand Hamlets)</li>\n<li>F2F, talking to Xian <strong>if the reviewer ever questioned about the evaluation procedures.</strong> (Why using storyboards as the evaluation materials can convince reviewers? For example, the \u201cCreate Sound\u201d style for opening, is hard to be presented in the storyboards. If so, how can experts / general audiences rated this style on data video only with the storyboard?)</li>\n</ul>",
        "update": "2024-9-22 10:18:24"
    },
    "4": {
        "name": "Yuying",
        "content": "<h1>\n<a name=\"paper-title-ai-paintings-vs-human-paintings-deciphering-public-interactions-and-perceptions-towards-ai-generated-paintings-on-tiktok-1\" class=\"anchor\" href=\"#paper-title-ai-paintings-vs-human-paintings-deciphering-public-interactions-and-perceptions-towards-ai-generated-paintings-on-tiktok-1\"></a>Paper Title: AI Paintings vs. Human Paintings? Deciphering Public Interactions and Perceptions Towards AI-Generated Paintings on TikTok</h1>\n<ul>\n<li>Conference/Journal : Arxiv 2024 (maybe submitted for CHI2025)</li>\n<li>Authors/group: Jiajun Wang, Xiangzhe Yuan, Siying Hu, Zhicong Lu</li>\n<li>Github link:</li>\n<li>Project page: <a href=\"https://arxiv.org/abs/2409.11911\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">[2409.11911] AI paintings vs. Human Paintings? Deciphering Public Interactions and Perceptions towards AI-Generated Paintings on TikTok</a>\n</li>\n</ul>\n<hr>\n<h2>\n<a name=\"spiral_notepad-summary-2\" class=\"anchor\" href=\"#spiral_notepad-summary-2\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/spiral_notepad.png?v=12\" title=\":spiral_notepad:\" class=\"emoji\" alt=\":spiral_notepad:\" loading=\"lazy\" width=\"20\" height=\"20\"> Summary</h2>\n<p>This paper contributes to understanding public interaction and perception towards AI-generated paintings (AIGP) on social media, particularly on TikTok. The study examines user engagement, and sentiment analysis of comments, and compares these between AIGP and human-generated paintings. Results indicate that human paintings generate more positive interactions and sentiment, despite the growing popularity of AI-generated content. Additionally, aesthetic quality does not significantly influence engagement. Negative perceptions of AIGP were further analyzed, revealing seven key reasons behind users\u2019 resistance, such as concerns over authenticity, artistic value, and a sense of unease towards AI\u2019s capabilities.</p>\n<hr>\n<h2>\n<a name=\"thinking-critical-thinking-3\" class=\"anchor\" href=\"#thinking-critical-thinking-3\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/thinking.png?v=12\" title=\":thinking:\" class=\"emoji\" alt=\":thinking:\" loading=\"lazy\" width=\"20\" height=\"20\"> Critical Thinking</h2>\n<h4>\n<a name=\"sparkles-positive-aspects-4\" class=\"anchor\" href=\"#sparkles-positive-aspects-4\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/sparkles.png?v=12\" title=\":sparkles:\" class=\"emoji\" alt=\":sparkles:\" loading=\"lazy\" width=\"20\" height=\"20\"> Positive Aspects</h4>\n<ul>\n<li>Idea: The focus on public perception of AI art on a fast-growing platform like TikTok addresses a contemporary issue, offering valuable insights into how generative AI impacts art consumption</li>\n<li>Project: The project demonstrates robust data collection, analyzing over 200 videos of AI and human-generated paintings, and tens of thousands of comments.</li>\n<li>Scalability: The approach can be extended to other social media platforms and forms of AI-generated content (e.g., music, literature)</li>\n<li>Evaluation: The study uses sophisticated tools such as LAION-Aesthetics V2 for assessing image quality, and an open-source model based on Distilbert for sentiment analysis</li>\n</ul>\n<h4>\n<a name=\"cloud_with_rain-negative-aspects-5\" class=\"anchor\" href=\"#cloud_with_rain-negative-aspects-5\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/cloud_with_rain.png?v=12\" title=\":cloud_with_rain:\" class=\"emoji\" alt=\":cloud_with_rain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Negative Aspects</h4>\n<ul>\n<li>System: Focusing on TikTok limits the generalizability of the findings to other platforms, where user behavior and preferences may differ significantly.</li>\n<li>Dataset:</li>\n</ul>\n<ol>\n<li>The dataset may lack representativeness due to platform-specific biases (e.g., younger audiences, TikTok\u2019s unique algorithm), limiting the application of these findings to a broader context.</li>\n<li>The search labels are limited, so it is unclear whether the selected videos are truly the most popular with the highest comments or if they were randomly chosen.</li>\n<li>What is the significance of removing duplicate images from AIGI videos? If there are repeated images in the videos, how would that impact the findings?</li>\n<li>The discrepancy between the initial collection of 2,667 videos and the final 219 is too large (over tenfold), raising questions about the reliability of the initial data collection method.</li>\n<li>Although the final number of AI image videos and human painting videos is similar, the data volume for the images is more than double, raising concerns about whether these are truly comparable and why there is such a big difference.</li>\n<li>After filtering with the aesthetic quality model, the videos are divided into four quality levels, but the significant disparity in video numbers between the 5.5-6.0 level makes the comparison\u2019s reliability questionable.</li>\n</ol>\n<ul>\n<li>Findings: The seven main reasons identified are rather ordinary and not particularly insightful, as they seem to reflect general societal consensus.</li>\n<li>Limitation: There seem to be some fundamental issues that may seriously impact the reliability of this research.</li>\n</ul>\n<hr>\n<h2>\n<a name=\"fountain-creative-thinking-6\" class=\"anchor\" href=\"#fountain-creative-thinking-6\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/fountain.png?v=12\" title=\":fountain:\" class=\"emoji\" alt=\":fountain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Creative thinking:</h2>\n<p>This study is relevant to my current attempt to research audience engagement with AI-generated videos, but I find many aspects of this paper is difficult for me to agree with. I hope to improve upon these aspects in my own research. However, I do agree that this study tries to control variables as much as possible to ensure a baseline comparison, which makes the research framework clear. But, the conclusions are not particularly compelling. In my research, I aim to draw more valuable conclusions, rather than findings that seem like common sense.</p>",
        "update": "2024-9-22 13:32:41"
    },
    "5": {
        "name": "JinduWang",
        "content": "<h1>\n<a name=\"paper-titleglanceable-ar-evaluating-information-access-methods-for-head-worn-augmented-reality-1\" class=\"anchor\" href=\"#paper-titleglanceable-ar-evaluating-information-access-methods-for-head-worn-augmented-reality-1\"></a>[Paper Title](Glanceable AR: Evaluating Information Access Methods for Head-Worn Augmented Reality)</h1>\n<ul>\n<li>Conference/Journal (IEEEVR-2020):</li>\n<li>Authors/group: Feiyu Lu, Shakiba Davari, Lee Lisle, Yuan Li, Doug A. Bowman (Virginia Tech)</li>\n</ul>\n<hr>\n<h2>\n<a name=\"spiral_notepad-summary-2\" class=\"anchor\" href=\"#spiral_notepad-summary-2\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/spiral_notepad.png?v=12\" title=\":spiral_notepad:\" class=\"emoji\" alt=\":spiral_notepad:\" loading=\"lazy\" width=\"20\" height=\"20\"> Summary</h2>\n<p>This paper proposed two novel hands-free interfaces called head-glance interfaces fixing virtual contents to user\u2019s body to be accessed by head rotation. Evaluations finds that the head-glance and eye-glance interfaces are more preferred and more efficient than the gaze-summon interface for discretionary information access. For a continuous monitoring task, the eye-glance interface was preferred.This is the first paper of the Glanceable AR paradigm series work. I will update this series.\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"//bbs.hkustvis.org/uploads/default/original/2X/4/49620106018ea9f20d52fd3a51626ad7546e1dd7.png\" data-download-href=\"//bbs.hkustvis.org/uploads/default/49620106018ea9f20d52fd3a51626ad7546e1dd7\" title=\"image\"><img src=\"//bbs.hkustvis.org/uploads/default/optimized/2X/4/49620106018ea9f20d52fd3a51626ad7546e1dd7_2_690x215.png\" alt=\"image\" data-base62-sha1=\"ataPkBfZ6HzYEFIvlE3xgc8Zq3t\" width=\"690\" height=\"215\" srcset=\"//bbs.hkustvis.org/uploads/default/optimized/2X/4/49620106018ea9f20d52fd3a51626ad7546e1dd7_2_690x215.png, //bbs.hkustvis.org/uploads/default/optimized/2X/4/49620106018ea9f20d52fd3a51626ad7546e1dd7_2_1035x322.png 1.5x, //bbs.hkustvis.org/uploads/default/original/2X/4/49620106018ea9f20d52fd3a51626ad7546e1dd7.png 2x\" data-dominant-color=\"E0E1E3\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1144\u00d7357 161 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<hr>\n<h2>\n<a name=\"thinking-critical-thinking-3\" class=\"anchor\" href=\"#thinking-critical-thinking-3\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/thinking.png?v=12\" title=\":thinking:\" class=\"emoji\" alt=\":thinking:\" loading=\"lazy\" width=\"20\" height=\"20\"> Critical Thinking</h2>\n<h4>\n<a name=\"sparkles-positive-aspects-4\" class=\"anchor\" href=\"#sparkles-positive-aspects-4\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/sparkles.png?v=12\" title=\":sparkles:\" class=\"emoji\" alt=\":sparkles:\" loading=\"lazy\" width=\"20\" height=\"20\"> Positive Aspects</h4>\n<ul>\n<li>\n<p>Idea: When this paper published, few papers were discussing about context-aware UIs in ARVR. For all-day AR users, how to easily access the multitask information is an important question. Right now many papers came out with many solutions for multitask scenarios to reduce the time from secondary task back to main tasks.\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"//bbs.hkustvis.org/uploads/default/original/2X/1/1b6c0535e655bf8b6e95d98961bb5dc1628b1f59.png\" data-download-href=\"//bbs.hkustvis.org/uploads/default/1b6c0535e655bf8b6e95d98961bb5dc1628b1f59\" title=\"image\"><img src=\"//bbs.hkustvis.org/uploads/default/original/2X/1/1b6c0535e655bf8b6e95d98961bb5dc1628b1f59.png\" alt=\"image\" data-base62-sha1=\"3UAk6o50Xm3LDzjXPQOcSpjL6FX\" width=\"690\" height=\"141\" data-dominant-color=\"E2E2E2\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">883\u00d7181 26.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n</li>\n<li>\n<p>Evaluation:\nPrimary task:\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"//bbs.hkustvis.org/uploads/default/original/2X/a/aa7a9fffd5be0dc32df3cd23e86da16ef0c357e5.jpeg\" data-download-href=\"//bbs.hkustvis.org/uploads/default/aa7a9fffd5be0dc32df3cd23e86da16ef0c357e5\" title=\"image\"><img src=\"//bbs.hkustvis.org/uploads/default/original/2X/a/aa7a9fffd5be0dc32df3cd23e86da16ef0c357e5.jpeg\" alt=\"image\" data-base62-sha1=\"ok7Tm0qzLTvhWW4Cr6Vm39w3dxH\" width=\"550\" height=\"500\" data-dominant-color=\"9FA0A0\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">783\u00d7711 75.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n</li>\n</ul>\n<p>Secondary task:\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"//bbs.hkustvis.org/uploads/default/original/2X/7/7b0999234ca7c26b8fcf99db6692e126dc5fd8e7.jpeg\" data-download-href=\"//bbs.hkustvis.org/uploads/default/7b0999234ca7c26b8fcf99db6692e126dc5fd8e7\" title=\"image\"><img src=\"//bbs.hkustvis.org/uploads/default/optimized/2X/7/7b0999234ca7c26b8fcf99db6692e126dc5fd8e7_2_690x160.jpeg\" alt=\"image\" data-base62-sha1=\"hyri9UXUgjJCDerfXQvhnyHWjA3\" width=\"690\" height=\"160\" srcset=\"//bbs.hkustvis.org/uploads/default/optimized/2X/7/7b0999234ca7c26b8fcf99db6692e126dc5fd8e7_2_690x160.jpeg, //bbs.hkustvis.org/uploads/default/optimized/2X/7/7b0999234ca7c26b8fcf99db6692e126dc5fd8e7_2_1035x240.jpeg 1.5x, //bbs.hkustvis.org/uploads/default/optimized/2X/7/7b0999234ca7c26b8fcf99db6692e126dc5fd8e7_2_1380x320.jpeg 2x\" data-dominant-color=\"8D9091\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1516\u00d7352 78.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Evaluation focus on performances of primary task   as well as secondary task performance, NASA-TLX and SUS. The interesting points are from the quilitative results, showing the shortcomings and advantages of the three UIs</p>\n<h4>\n<a name=\"cloud_with_rain-negative-aspects-5\" class=\"anchor\" href=\"#cloud_with_rain-negative-aspects-5\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/cloud_with_rain.png?v=12\" title=\":cloud_with_rain:\" class=\"emoji\" alt=\":cloud_with_rain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Negative Aspects</h4>\n<ul>\n<li>Design:\nThere is always a question in my mind about the doubt of \u201cFor example, if users are engaged in a high-level task, a low-level\nambient output is desired\u201d. Currently many context-aware papers evaluated the time reduction from the secondary task back to the main task, in stead of focusing on the efficiency of the main task itself. Does it really matter, maybe for walking situations there are some concerns for safety, what about workspace? Do we really need the faster coming back?</li>\n</ul>\n<p>This paper purposed an easy  evaluations and provided some initial findings for the later Glanceable AR works.</p>\n<hr>\n<h2>\n<a name=\"fountain-creative-thinking-6\" class=\"anchor\" href=\"#fountain-creative-thinking-6\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/fountain.png?v=12\" title=\":fountain:\" class=\"emoji\" alt=\":fountain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Creative thinking:</h2>\n<p>As mentioned above, the performance of the primary task showed no significant difference. A context-aware system contains input and output two parts. Maybe the output is not the key factor effecting the main task?</p>\n<p>There are some context-aware systems focus on main task performances, but with a narrow down situation. If we can find the proper inputs and outputs based on the key factors matter the general task abilities proved cognitive science findings.</p>\n<p>After all, I will keep reading papers related to context-aware systems and update them here.</p>",
        "update": "2024-9-20 7:51:40"
    },
    "6": {
        "name": "Zheng Wei",
        "content": "<h1>\n<a name=\"paper-titlethe-effects-of-avatar-and-environment-design-1\" class=\"anchor\" href=\"#paper-titlethe-effects-of-avatar-and-environment-design-1\"></a>[Paper Title](The Effects of Avatar and Environment Design</h1>\n<p>on Embodiment, Presence, Activation, and Task Load\nin a Virtual Reality Exercise Application)</p>\n<ul>\n<li>Conference/Journal (Name-Year):</li>\n<li>Authors/group: <a href=\"https://ieeexplore.ieee.org/author/37086574680\" rel=\"noopener nofollow ugc\">Andrea Bartl</a>; <a href=\"https://ieeexplore.ieee.org/author/37088537317\" rel=\"noopener nofollow ugc\">Christian Merz</a>; <a href=\"https://ieeexplore.ieee.org/author/37085782390\" rel=\"noopener nofollow ugc\">Daniel Roth</a>; <a href=\"https://ieeexplore.ieee.org/author/37282895600\" rel=\"noopener nofollow ugc\">Marc Erich Latoschik</a>\n</li>\n<li>Github link: <a href=\"https://ieeexplore.ieee.org/document/9994882/\" rel=\"noopener nofollow ugc\">https://ieeexplore.ieee.org/document/9994882/</a>\n</li>\n<li>Project page:</li>\n</ul>\n<hr>\n<h2>\n<a name=\"spiral_notepad-summary-2\" class=\"anchor\" href=\"#spiral_notepad-summary-2\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/spiral_notepad.png?v=12\" title=\":spiral_notepad:\" class=\"emoji\" alt=\":spiral_notepad:\" loading=\"lazy\" width=\"20\" height=\"20\"> Summary</h2>\n<h2>\n<a name=\"this-article-examines-how-design-choices-in-embodied-virtual-reality-vr-systems-affect-user-perception-focusing-on-factors-like-sense-of-embodiment-presence-motivation-activation-and-task-load-in-a-vr-physical-exercise-application-two-user-studies-were-conducted-the-first-manipulated-avatar-fidelity-abstract-vs-full-body-and-the-presence-of-a-mirror-revealing-that-full-body-avatars-enhance-embodiment-and-reduce-mental-demand-while-mirrors-had-no-significant-effect-the-second-study-varied-avatar-type-healthy-vs-injured-and-environment-beach-vs-hospital-finding-that-injured-avatars-increased-temporal-demand-and-that-beach-environments-reduced-tense-activation-interestingly-participants-felt-more-present-while-embodying-the-injured-avatar-in-the-beach-setting-despite-the-incongruence-3\" class=\"anchor\" href=\"#this-article-examines-how-design-choices-in-embodied-virtual-reality-vr-systems-affect-user-perception-focusing-on-factors-like-sense-of-embodiment-presence-motivation-activation-and-task-load-in-a-vr-physical-exercise-application-two-user-studies-were-conducted-the-first-manipulated-avatar-fidelity-abstract-vs-full-body-and-the-presence-of-a-mirror-revealing-that-full-body-avatars-enhance-embodiment-and-reduce-mental-demand-while-mirrors-had-no-significant-effect-the-second-study-varied-avatar-type-healthy-vs-injured-and-environment-beach-vs-hospital-finding-that-injured-avatars-increased-temporal-demand-and-that-beach-environments-reduced-tense-activation-interestingly-participants-felt-more-present-while-embodying-the-injured-avatar-in-the-beach-setting-despite-the-incongruence-3\"></a>This article examines how design choices in embodied Virtual Reality (VR) systems affect user perception, focusing on factors like sense of embodiment, presence, motivation, activation, and task load in a VR physical exercise application. Two user studies were conducted: the first manipulated avatar fidelity (abstract vs. full-body) and the presence of a mirror, revealing that full-body avatars enhance embodiment and reduce mental demand, while mirrors had no significant effect. The second study varied avatar type (healthy vs. injured) and environment (beach vs. hospital), finding that injured avatars increased temporal demand and that beach environments reduced tense activation. Interestingly, participants felt more present while embodying the injured avatar in the beach setting, despite the incongruence.</h2>\n<h2>\n<a name=\"thinking-critical-thinking-4\" class=\"anchor\" href=\"#thinking-critical-thinking-4\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/thinking.png?v=12\" title=\":thinking:\" class=\"emoji\" alt=\":thinking:\" loading=\"lazy\" width=\"20\" height=\"20\"> Critical Thinking</h2>\n<h4>\n<a name=\"sparkles-positive-aspects-5\" class=\"anchor\" href=\"#sparkles-positive-aspects-5\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/sparkles.png?v=12\" title=\":sparkles:\" class=\"emoji\" alt=\":sparkles:\" loading=\"lazy\" width=\"20\" height=\"20\"> Positive Aspects</h4>\n<p>Environment Influence: The findings highlight the importance of environment in VR experiences, demonstrating how different settings can affect user engagement and emotional response.\nUser-Centric Design: By focusing on user perception and experience, the study emphasizes the need for thoughtful design choices in developing VR applications.\nPractical Implications: The results can guide developers in creating more effective and engaging VR applications for physical exercise and rehabilitation.\nDual Study Approach: Utilizing two separate studies allows for a comprehensive exploration of various factors, enhancing the reliability of the findings.\nAwareness of Congruence: The research underscores the significance of avatar-environment congruence, contributing to a deeper understanding of how mismatches can impact user experience.</p>\n<h4>\n<a name=\"cloud_with_rain-negative-aspects-6\" class=\"anchor\" href=\"#cloud_with_rain-negative-aspects-6\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/cloud_with_rain.png?v=12\" title=\":cloud_with_rain:\" class=\"emoji\" alt=\":cloud_with_rain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Negative Aspects</h4>\n<p>Limited Generalizability: The findings may not be applicable to all VR contexts or user populations, as the studies focus on specific settings and avatar types.\nInconsistent Mirror Effects: The lack of significant findings regarding the mirror\u2019s influence could indicate a need for further investigation or different experimental designs.\nNarrow Focus: By concentrating primarily on embodiment and mental demand, other important factors (like long-term user engagement) might be overlooked.\nTemporal Demand: The increased temporal demand with the injured avatar might suggest potential frustration for users, impacting their overall experience negatively.</p>\n<hr>\n<h2>\n<a name=\"fountain-creative-thinking-7\" class=\"anchor\" href=\"#fountain-creative-thinking-7\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/fountain.png?v=12\" title=\":fountain:\" class=\"emoji\" alt=\":fountain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Creative thinking:</h2>\n<p>Hybrid Environments: Designing dynamic VR environments that change based on user performance or emotional state, enhancing engagement.\nPersonalized Avatars: Allowing users to customize their avatars based on personal preferences or fitness levels, fostering a stronger connection.</p>",
        "update": "2024-9-22 13:43:42"
    },
    "7": {
        "name": "Jiayi Zhou",
        "content": "<p>iterate the template inspired by <a class=\"mention\" href=\"/u/linping\">@linping</a></p>\n<h1>\n<a name=\"narrative-visualization-telling-stories-with-data-1\" class=\"anchor\" href=\"#narrative-visualization-telling-stories-with-data-1\"></a>Narrative Visualization: Telling Stories With Data</h1>\n<p>Note: this paper contains valuable details that are worth revisiting</p>\n<p><a href=\"https://www.floodbase.com/team-members/eddie-segel\" rel=\"noopener nofollow ugc\">*Edward Segel</a> and <a href=\"https://scholar.google.com.hk/citations?user=vlgs4G4AAAAJ&amp;hl=zh-CN&amp;oi=sra\" rel=\"noopener nofollow ugc\">Jeffrey Heer</a>*</p>\n<p>TVCG 2010 (Number of citations: 1800)</p>\n<p><strong>Keywords</strong>: Narrative Visualization, case study, design space, design strategies</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"//bbs.hkustvis.org/uploads/default/original/2X/d/d5a5f3f057d5f8635035d194a0d462f32e5b328d.png\" data-download-href=\"//bbs.hkustvis.org/uploads/default/d5a5f3f057d5f8635035d194a0d462f32e5b328d\" title=\"image\"><img src=\"//bbs.hkustvis.org/uploads/default/optimized/2X/d/d5a5f3f057d5f8635035d194a0d462f32e5b328d_2_471x500.png\" alt=\"image\" data-base62-sha1=\"uu1fIVM74n6PjjLqpzofWMqyOpD\" width=\"471\" height=\"500\" srcset=\"//bbs.hkustvis.org/uploads/default/optimized/2X/d/d5a5f3f057d5f8635035d194a0d462f32e5b328d_2_471x500.png, //bbs.hkustvis.org/uploads/default/optimized/2X/d/d5a5f3f057d5f8635035d194a0d462f32e5b328d_2_706x750.png 1.5x, //bbs.hkustvis.org/uploads/default/optimized/2X/d/d5a5f3f057d5f8635035d194a0d462f32e5b328d_2_942x1000.png 2x\" data-dominant-color=\"98B5ED\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1236\u00d71312 219 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<h3>\n<a name=\"summary-2\" class=\"anchor\" href=\"#summary-2\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/writing_hand/2.png?v=12\" title=\":writing_hand:t2:\" class=\"emoji\" alt=\":writing_hand:t2:\" loading=\"lazy\" width=\"20\" height=\"20\"> Summary</h3>\n<p><strong>[M]</strong> Static data visualization has long been an essential backbone for stories, and recently, journalists, politicians, activists, and television reporters have increasingly created interactive data visualization to support storytelling. Such \u2018data stories\u2019 differ in important ways from traditional storytelling. <strong>[P]</strong> However, it remains an open question how the design of visualization tools might evolve to support richer and more diverse forms of storytelling. <strong>[Me]</strong> Analyzing 58 examples and drawing from 5 cases, this paper formulates a design space concerning genre, visual narrative tactic, and narrative structure tactic. <strong>[C]</strong> In particular, authors observe the central concern of balancing author-driven and reader-driven elements and discuss three common schemas.</p>\n<h3>\n<a name=\"critical-thinking-3\" class=\"anchor\" href=\"#critical-thinking-3\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/thinking.png?v=12\" title=\":thinking:\" class=\"emoji\" alt=\":thinking:\" loading=\"lazy\" width=\"20\" height=\"20\"> Critical Thinking</h3>\n<div class=\"md-table\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>\n<img src=\"//bbs.hkustvis.org/images/emoji/twitter/+1/2.png?v=12\" title=\":+1:t2:\" class=\"emoji\" alt=\":+1:t2:\" loading=\"lazy\" width=\"20\" height=\"20\"> Pros</th>\n<th>\n<img src=\"//bbs.hkustvis.org/images/emoji/twitter/-1/2.png?v=12\" title=\":-1:t2:\" class=\"emoji\" alt=\":-1:t2:\" loading=\"lazy\" width=\"20\" height=\"20\"> Cons</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>[M]otivation</strong></td>\n<td>keenly aware and clearly point out the real market for data stories (politics, education, journalism, art)</td>\n<td>should have described the uniqueness of data stories in comparison to traditional stories</td>\n</tr>\n<tr>\n<td><strong>Research [P]roblem</strong></td>\n<td>a real problem</td>\n<td>lack clarity, transition from M to P is not smooth enough</td>\n</tr>\n<tr>\n<td><strong>[M]ethod</strong></td>\n<td>solid and enough for observation</td>\n<td>lack of rigorous and clear logic between related works, cases, and selected examples</td>\n</tr>\n<tr>\n<td><strong>[C]ontribution to community</strong></td>\n<td>profound</td>\n<td>lack of rigorous and clear logic in explaining several contributions ( could use O1-On)</td>\n</tr>\n<tr>\n<td><strong>Writing</strong></td>\n<td>wording, analyzing</td>\n<td>logic</td>\n</tr>\n<tr>\n<td><strong>Picture</strong></td>\n<td>clear and cute, support for texts</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div><h3>\n<a name=\"creative-thinking-4\" class=\"anchor\" href=\"#creative-thinking-4\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/bulb.png?v=12\" title=\":bulb:\" class=\"emoji\" alt=\":bulb:\" loading=\"lazy\" width=\"20\" height=\"20\"> Creative Thinking</h3>\n<ol>\n<li>think about the definition of data stories/data storytelling/narrative visualization and its evolution across time</li>\n<li>think about the uniqueness of data stories in comparison to traditional stories</li>\n<li>collect cases of data stories and create a database (any existing work?)</li>\n<li>can try to reorganize this paper to make it clearer and logical.</li>\n</ol>\n<h3>\n<a name=\"what-i-have-learned-5\" class=\"anchor\" href=\"#what-i-have-learned-5\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/brain.png?v=12\" title=\":brain:\" class=\"emoji\" alt=\":brain:\" loading=\"lazy\" width=\"20\" height=\"20\"> What I have Learned</h3>\n<ul>\n<li>how to analyze a case in terms of data storytelling</li>\n<li>the possible market for data stories</li>\n</ul>",
        "update": "2024-9-22 4:57:58"
    },
    "8": {
        "name": "Wenshuo ZHANG",
        "content": "<h1>\n<a name=\"paper-title-sugar-surface-aligned-gaussian-splatting-for-efficient-3d-mesh-reconstruction-and-rendering-1\" class=\"anchor\" href=\"#paper-title-sugar-surface-aligned-gaussian-splatting-for-efficient-3d-mesh-reconstruction-and-rendering-1\"></a>[Paper Title]( SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and Rendering)</h1>\n<ul>\n<li>Conference/Journal (Name-Year): CVPR2024</li>\n<li>Authors/group: <a href=\"https://ieeexplore.ieee.org/author/37089993917\" rel=\"noopener nofollow ugc\">Antoine Gu\u00e9don</a>; <a href=\"https://ieeexplore.ieee.org/author/515660656105871\" rel=\"noopener nofollow ugc\">Vincent Lepetit</a>\n</li>\n<li>Github link: <a href=\"https://ieeexplore.ieee.org/author/37089993917\" rel=\"noopener nofollow ugc\">Antoine Gu\u00e9don</a>; <a href=\"https://ieeexplore.ieee.org/author/515660656105871\" rel=\"noopener nofollow ugc\">Vincent Lepetit</a>\n</li>\n</ul>\n<hr>\n<h2>\n<a name=\"spiral_notepad-summary-2\" class=\"anchor\" href=\"#spiral_notepad-summary-2\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/spiral_notepad.png?v=12\" title=\":spiral_notepad:\" class=\"emoji\" alt=\":spiral_notepad:\" loading=\"lazy\" width=\"20\" height=\"20\"> Summary</h2>\n<p>\u6587\u7ae0\u94fe\u63a5\u662f\uff1a<a href=\"https://link.zhihu.com/?target=https%3A//ieeexplore.ieee.org/document/10655755\" rel=\"noopener nofollow ugc\">SuGaR\uff1a\u8868\u9762\u5bf9\u9f50\u9ad8\u65af\u5c55\u5f00\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684 3D \u7f51\u683c\u91cd\u5efa\u548c\u9ad8\u8d28\u91cf\u7f51\u683c\u6e32\u67d3 |IEEE \u4f1a\u8bae\u51fa\u7248\u7269 |IEEE Xplore</a></p>\n<p>\u5b9e\u9645\u4e0a\u5c31\u662f\uff1aSuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering\u8fd9\u4e2a\u6587\u7ae0\u7684\u3002\u8fd9\u4e2a\u662f\u7528\u6765\u8bb23DGS\u751f\u6210\u7684\u7ed3\u679c\u8f6c\u53d8\u6210mesh\u7684\uff0c\u8f6c\u53d8\u5b8c\u6210\u4e4b\u540e\u5c31\u53ef\u4ee5\u8fdb\u884c\u5bf9\u4e8eobject\u7684\u64cd\u4f5c\u7528\u4e8e\u5b8c\u62103DGS\u5230VR\u73af\u5883\u7684\u8fd9\u4e00\u6b65\u3002</p>\n<p>\u6211\u4eec\u8fd9\u4e2a\u5730\u65b9\u4f1a\u505a\u4e00\u4e2a\u7cfb\u5217\u7684\u6587\u7ae0\uff0c\u8fd9\u4e2a\u6587\u7ae0\u4f1a\u5305\u542b\u591a\u4e2a\u5185\u5bb9\uff1a\u4f20\u7edf\u7684\uff1bSuGaR\u4ee5\u53ca\u4e00\u4e9b\u65b0\u7684\u5185\u5bb9</p>\n<p>\u8fd9\u4e2aconvert\u662f\u6bd4\u8f83\u96be\u7684\uff1a1. point cloud\u6bd4\u8f83\u590d\u6742\uff0c2. \u5176\u6b21\u4ed6\u8fd9\u4e2a\u7684\u989c\u8272\u8868\u5f81\u7528\u7684\u662f\u7403\u8c10\u51fd\u6570\uff0c\u6240\u4ee5\u5f88\u96be\u76f4\u63a5\u8f6c\u6362\u3002\u8fd9\u4e24\u4e2a\u76f4\u63a5\u5bfc\u81f4\u9ad8\u65af\u5206\u5e03\u4e0e\u573a\u666f\u8868\u9762\u65e0\u6cd5\u5bf9\u9f50\u3002</p>\n<p>\u4e3a\u89e3\u51b3\u8fd9\u4e2a\uff0c\u4ed6\u4eec\u63d0\u51fa\u4e86\u51e0\u4e2a\u70b9\uff1a1.\u6b63\u5219\u5316\u9879\u8ba9point\u51fa\u73b0\u5728\u8868\u9762\u4ee5\u4fbf\u4e8e\u53d1\u73b0mesh\uff0c\u4e3a\u4e86\u8bc4\u4f30\u662f\u5426\u6b63\u5219\u5316\uff0c\u4ed6\u4eec\u7ed9\u4e86\u4e00\u4e2a\u51fd\u6570\u7528\u4e8e\u8861\u91cf\u5bc6\u5ea6\u30022.\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\u7528\u4e8e\u5feb\u901f\u63d0\u53d6mesh\u30023.\u5c06\u8fd9\u4e24\u8005\u7ed1\u5b9a\u5728\u4e00\u8d77\u5f62\u6210\u7cbe\u7ec6\u7f51\u7edc</p>\n<p>\u9884\u5148\u76843DGS\u8bf7\u770b\uff1a<a href=\"https://zhuanlan.zhihu.com/p/719837176\" rel=\"noopener nofollow ugc\">3D Gaussian Splatting for Real-Time Radiance Field Rendering - \u77e5\u4e4e (zhihu.com)</a>\u3001</p>\n<h3>\n<a name=\"h-3\" class=\"anchor\" href=\"#h-3\"></a>\u6b63\u5219\u5316\u8868\u8fbe</h3>\n<p>&lt;1&gt;\u521d\u59cb\u7684\u5f62\u5f0f\u5316\u8bba\u8bc1</p>\n<p>\u73b0\u5728\u6211\u4eec\u6bcf\u4e00\u4e2a\u70b9\u4e0a\u7684\u5bc6\u5ea6\u90fd\u662f\u6bcf\u4e00\u4e2a\u9ad8\u65af\u70b9\u7684\u7d2f\u52a0\uff1a</p>\n<p><img src=\"https://pic4.zhimg.com/80/v2-112ec063cabe6edde6f885fa2f8d2b4f_720w.webp\" alt=\"\" width=\"538\" height=\"133\" role=\"presentation\"></p>\n<p>\u5bf9\u4e8e\u6700\u5916\u4fa7\u7684\u70b9\u800c\u8a00\uff0c\u5e94\u8be5\u5360\u636e\u7684\u6743\u91cd\u662f\u6700\u5927\u7684\uff0c\u4e5f\u5c31\u662f\u6700\u5916\u4fa7\u7684\u70b9g*\u5bf9\u4e8e\u6700\u540e\u7ed3\u679c\u7684contribution\u6700\u5927\u5316\uff1a</p>\n<p><img src=\"https://pic3.zhimg.com/80/v2-b76d2f87a6bab3359513b7200ade677a_720w.webp\" alt=\"\" width=\"379\" height=\"64\" role=\"presentation\"></p>\n<p>\u8fd9\u4e2a\u65f6\u5019\u7684g*\u7684\u5b9a\u4e49\u5982\u4e0b\uff0c\u5c31\u662f\u6700\u5927\u8d21\u732e\u70b9\u7684\u5f62\u5f0f\u5316\u8868\u8fbe\uff1a</p>\n<p><img src=\"https://pic4.zhimg.com/80/v2-ad8e12e71b3e507ea14c9b5df90f1971_720w.webp\" alt=\"\" width=\"357\" height=\"60\" role=\"presentation\"></p>\n<p>&lt;2&gt;\u540e\u7eed\u64cd\u4f5c\uff1a\u6241\u5e73\u5316\u9ad8\u65af</p>\n<p>\u7814\u7a76\u56e2\u961f\u5e0c\u671b3D\u9ad8\u65af\u5206\u5e03\u662f\u6241\u5e73\u7684\uff0c\u56e0\u4e3a\u8fd9\u6837\u5b83\u4eec\u5c31\u80fd\u66f4\u7d27\u5bc6\u5730\u4e0e\u7f51\u683c\u7684\u8868\u9762\u5bf9\u9f50\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u6bcf\u4e00\u4e2a\u9ad8\u65af\u5206\u5e03g<em>g</em>\u7684\u4e09\u4e2a\u5c3a\u5ea6\u56e0\u5b50\u4e2d\u7684\u4e00\u4e2a\u5e94\u8be5\u63a5\u8fd1\u4e8e0\u3002\u8fd9\u662f\u56e0\u4e3a\u5f53\u4e00\u4e2a\u9ad8\u65af\u5206\u5e03\u53d8\u5f97\u6241\u5e73\u65f6\uff0c\u5b83\u7684\u4e00\u4e2a\u4e3b\u8f74\u65b9\u5411\u5c06\u4f1a\u51e0\u4e4e\u5e73\u884c\u4e8e\u7f51\u683c\u7684\u8868\u9762\uff0c\u8fd9\u6709\u52a9\u4e8e\u9ad8\u65af\u5206\u5e03\u66f4\u597d\u5730\u63cf\u8ff0\u573a\u666f\u8868\u9762\u7684\u7ec6\u8282\u3002\u6700\u540e\u5f62\u6210\u4e86\u4e0b\u8fb9\u7684\uff1a</p>\n<p><img src=\"https://pic2.zhimg.com/80/v2-7d96e50276de5917fcb2b07ac928f0ad_720w.webp\" alt=\"\" width=\"410\" height=\"76\" role=\"presentation\"></p>\n<p>\u8fd9\u4e2a\u5730\u65b9\u4ed6\u5e0c\u671b\u6700\u540e\u7684\u7ed3\u679c\u662f\u4e0d\u900f\u660e\u7684\uff0c\u6240\u4ee5\u03b1\u90fd\u5199\u62101\uff0c\u6700\u540eflatten\u4e4b\u540e\u5c31\u53ea\u5269\u4e0b\u4e0d\u900f\u660e\u7684\u6700\u540e\u4e00\u5c42\u3002\u8fd9\u4e2a\u65f6\u5019\u5316\u7b80\u5b8c\u4e4b\u540e\u5c31\u662f\uff1a</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-442f80be1e4a69e56a72eff94b81ba32_720w.webp\" alt=\"\" width=\"358\" height=\"82\" role=\"presentation\"></p>\n<p>\u8fd9\u4e2a\u65f6\u5019\u7684d/\u5c31\u662f\u6700\u4f18\u7684\u76ee\u6807\uff0c\u6240\u4ee5\u53ea\u9700\u8981\u8ba9\u6e32\u67d3\u51fa\u6765\u7684\u7ed3\u679c\u548c\u6700\u4f18\u76ee\u6807\u63a5\u8fd1\u5c31\u884c\uff0c\u4e5f\u5c31\u662f\u8bf4\u6700\u5c0f\u5316\u4e0b\u8fb9\u7684function\u5373\u53ef\uff1a</p>\n<p><img src=\"https://pica.zhimg.com/80/v2-1681f2d212df4766df10e2f175563fee_720w.webp\" alt=\"\" width=\"121\" height=\"36\" role=\"presentation\"></p>\n<p>&lt;3&gt;Signed Distance Function\uff08\u6709\u6b63\u8d1f\u7684\uff09</p>\n<p>\u4ed6\u4eec\u5728\u505a\u5b8c\u4e0a\u8fb9\u7684\u5de5\u4f5c\u4e4b\u540e\u53d1\u73b0\u4f7f\u7528\u4e00\u4e2a\u65b0\u7684\u51fd\u6570\u53ef\u4ee5\u66f4\u597d\u7684\u4f18\u5316\u3002\u539f\u56e0\u662f\u6700\u5916\u8fb9\u7684\u4e00\u5c42\u7684g*\u548c\u8868\u9762\u5176\u5b9e\u8fd8\u6709\u8ddd\u79bb\uff0c\u8fd9\u4e2a\u8ddd\u79bb\u4f1a\u5f15\u5165bias\u3002</p>\n<p>SDF\uff1a\u7b7e\u540d\u8ddd\u79bb\u51fd\u6570\u662f\u4e00\u4e2a\u6807\u91cf\u573a\uff0c\u5b83\u5728\u6bcf\u4e00\u70b9\u4e0a\u7ed9\u51fa\u8be5\u70b9\u5230\u6700\u8fd1\u7684\u8868\u9762\u7684\u8ddd\u79bb\u3002\u5982\u679c\u8be5\u70b9\u4f4d\u4e8e\u8868\u9762\u5185\u90e8\uff0c\u8be5\u503c\u53d6\u8d1f\uff1b\u5982\u679c\u4f4d\u4e8e\u5916\u90e8\uff0c\u5219\u53d6\u6b63\u3002</p>\n<p>\u5bc6\u5ea6\uff1a\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u5bc6\u5ea6\u662f\u6307\u9ad8\u65af\u5206\u5e03\u7684\u7d2f\u79ef\u6548\u5e94\uff0c\u5b83\u8868\u793a\u5728\u7279\u5b9a\u70b9\u5904\u7684\u7269\u8d28\u6d53\u5ea6\u3002</p>\n<p>\u6211\u4eec\u8bb2\u8fd9\u4e2a\u6700\u540e\u7684\u4e00\u4e2a\u9ad8\u65af\u70b9\u7684\u534a\u5f84\u8bb0\u4e3as\u3002\u90a3\u5e94\u7528\u8ddd\u79bb\u957f\u4e4b\u540e\u7684\u7ed3\u679c\u662f\uff1a</p>\n<p><img src=\"https://picx.zhimg.com/80/v2-2d7923cc7443e7e335c2e1985544a8bb_720w.webp\" alt=\"\" width=\"281\" height=\"60\" role=\"presentation\"></p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-3aab4940725113fcd6e5d89562840d4c_720w.webp\" alt=\"\" width=\"286\" height=\"53\" role=\"presentation\"></p>\n<p>\u8fd9\u4e24\u4e2a\u4e00\u4e2a\u662f\u76ee\u6807\u7684\u6807\u51c6\u8ddd\u79bb\uff0c\u4e00\u4e2a\u5f53\u524d\u9ad8\u65af\u7684\u5b9e\u9645\u8ddd\u79bb\uff0c\u7136\u540e\u4f18\u5316\u8fd9\u4e24\u4e2a\u5c31\u884c</p>\n<p><img src=\"https://pica.zhimg.com/80/v2-03e3421917b91328519bafc5334f0994_720w.webp\" alt=\"\" width=\"315\" height=\"80\" role=\"presentation\"></p>\n<p>\u56e0\u4e3a\u6700\u4f18\u76ee\u6807\u6240\u6784\u6210\u7684\u8868\u9762\u518df\u8ba1\u7b97\u7684\u65f6\u5019\u4e00\u76f4\u662f0\uff0c\u6240\u4ee5\u6709\u6b63\u8d1f\u7684\u8ddd\u79bb\u8ba1\u7b97\u51fd\u6570\u6709\u70b9\u591a\u6b64\u4e00\u4e3e\uff0c\u56e0\u800c\u76f4\u63a5\u4f7f\u7528\u4e0d\u8ba1\u6b63\u8d1f\u7684\u5185\u5bb9,\u4f7f\u7528unsigned Distance Function\uff08\u6ca1\u6709\u6b63\u8d1f\u7684\uff09</p>\n<p>&lt;4&gt;SDF\u7684\u8ba1\u7b97</p>\n<p>\u9ad8\u6548\u5730\u8ba1\u7b97 f^(p)<em>f</em>^\u200b(<em>p</em>)\uff0c\u5373\u4f30\u8ba1\u7684SDF\u51fd\u6570\u5728\u70b9 p\u5904\u7684\u503c\uff0c\u6700\u521d\u662f\u5177\u6709\u6311\u6218\u6027\u7684\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e2a\u6311\u6218\uff0c\u7814\u7a76\u8005\u63d0\u8bae\u4f7f\u7528\u4ece\u8bad\u7ec3\u89c6\u89d2\u83b7\u53d6\u7684\u9ad8\u65af\u5206\u5e03\u7684\u6df1\u5ea6\u56fe\u3002\u8fd9\u4e9b\u6df1\u5ea6\u56fe\u53ef\u4ee5\u901a\u8fc7\u6269\u5c55\u70b9\u7ed8\u5236\u5668\uff08splatting rasterizer\uff09\u6765\u9ad8\u6548\u5730\u751f\u6210\u3002</p>\n<p>\u5176\u4e2d\uff0c\u6269\u5c55\u70b9\u7ed8\u5236\u5668\uff08extended splatting rasterizer\uff09\uff0c\u5728\u8fd9\u4e2a\u4e0a\u4e0b\u6587\u4e2d\u6307\u7684\u662f\u4e00\u4e2a\u7ecf\u8fc7\u6539\u8fdb\u7684\u7a0b\u5e8f\u6216\u7b97\u6cd5\u6a21\u5757\uff0c\u5b83\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u548c\u6e32\u67d3\u6765\u81ea\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u7684\u9ad8\u65af\u5206\u5e03\u6570\u636e\u5230\u4e8c\u7ef4\u56fe\u50cf\u5e73\u9762\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5728SuGaR\u65b9\u6cd5\u4e2d\uff0c\u5b83\u5c06\u4e09\u7ef4\u9ad8\u65af\u5206\u5e03\u8f6c\u6362\u6210\u5e73\u884c\u4e8e\u56fe\u50cf\u5e73\u9762\u7684\u4e8c\u7ef4\u9ad8\u65af\u5206\u5e03\u6765\u8fdb\u884c\u6e32\u67d3\uff0c\u8fd9\u4f7f\u5f97\u6e32\u67d3\u8fc7\u7a0b\u6781\u4e3a\u5feb\u901f\u3002</p>\n<p>\u63a5\u7740\uff0c\u5bf9\u4e8e\u4e00\u4e2a\u4ece\u8bad\u7ec3\u89c6\u89d2\u53ef\u89c1\u7684\u70b9 p\uff0cf^(p)<em>f</em>^\u200b(<em>p</em>) \u5b9a\u4e49\u4e3a p\u7684\u6df1\u5ea6\u4e0e\u5728 p\u6295\u5f71\u4f4d\u7f6e\u5904\u5bf9\u5e94\u7684\u6df1\u5ea6\u56fe\u4e2d\u7684\u6df1\u5ea6\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u4e5f\u5c31\u662f\u8bf4\uff0cf^(p)<em>f</em>^\u200b(<em>p</em>) \u662f\u901a\u8fc7\u6d4b\u91cf\u70b9 p \u5728\u6df1\u5ea6\u56fe\u4e2d\u7684\u6295\u5f71\u4f4d\u7f6e\u7684\u6df1\u5ea6\u4e0e\u70b9 p\u5b9e\u9645\u4f4d\u7f6e\u7684\u6df1\u5ea6\u4e4b\u95f4\u7684\u5dee\u5f02\u6765\u4f30\u8ba1\u7684\u3002</p>\n<h3>\n<a name=\"h-4\" class=\"anchor\" href=\"#h-4\"></a>\u9ad8\u6548\u7684\u7f51\u683c\u63d0\u53d6</h3>\n<ol>\n<li>\u91c7\u68373D\u70b9\uff1a\u4ece\u9ad8\u65af\u5206\u5e03\u8ba1\u7b97\u5f97\u5230\u7684\u5bc6\u5ea6\u7684\u4e00\u4e2a\u6c34\u5e73\u96c6\u4e2d\u91c7\u68373D\u70b9\u3002\u6c34\u5e73\u96c6\u7684\u786e\u5b9a\u4f9d\u8d56\u4e8e\u4e00\u4e2a\u9608\u503c\u03bb\u3002</li>\n<li>\u6cca\u677e\u91cd\u5efa\uff1a\u4f7f\u7528\u6cca\u677e\u91cd\u5efa\u7b97\u6cd5\u6765\u521b\u5efa\u7f51\u683c\uff0c\u8be5\u7b97\u6cd5\u63a5\u6536\u6c34\u5e73\u96c6\u4e0a\u7684\u70b9\u5e76\u6784\u5efa\u7f51\u683c\u3002\u6b64\u5916\uff0c\u8fd8\u53ef\u4ee5\u4e3a\u8fd9\u4e9b\u70b9\u5206\u914dSDF\u7684\u6cd5\u7ebf\uff0c\u8fd9\u53ef\u4ee5\u63d0\u9ad8\u7f51\u683c\u7684\u8d28\u91cf\u3002</li>\n<li>\u9ad8\u6548\u8bc6\u522b\u70b9\uff1a\u4e3a\u4e86\u6709\u6548\u5730\u627e\u51fa\u4f4d\u4e8e\u6c34\u5e73\u96c6\u4e0a\u7684\u70b9\uff0c\u4f7f\u7528\u4e86\u9ad8\u65af\u5206\u5e03\u7684\u6df1\u5ea6\u56fe\u3002\u4ece\u6bcf\u4e2a\u6df1\u5ea6\u56fe\u4e2d\u968f\u673a\u9009\u53d6\u50cf\u7d20\u70b9\uff0c\u5e76\u9488\u5bf9\u6bcf\u4e2a\u50cf\u7d20\u70b9\u91c7\u6837\u5176\u89c6\u7ebf\u65b9\u5411\uff0c\u5bfb\u627e\u4f4d\u4e8e\u6c34\u5e73\u96c6\u4e0a\u76843D\u70b9\u3002</li>\n<li>\u91c7\u6837\u8fc7\u7a0b\uff1a\u5bf9\u4e8e\u6bcf\u4e2a\u50cf\u7d20\u70b9\uff0c\u91c7\u6837n\u4e2a\u70b9p + tiv\uff0c\u5176\u4e2dp\u662f\u4ece\u6df1\u5ea6\u56fe\u4e2d\u91cd\u65b0\u6295\u5f71\u5230\u8be5\u50cf\u7d20\u70b9\u76843D\u70b9\uff0cv\u662f\u89c6\u7ebf\u7684\u65b9\u5411\uff0cti\u7684\u8303\u56f4\u662f[-3\u03c3g(v),3\u03c3g(v)]\uff0c\u03c3g(v)\u662f\u9ad8\u65af\u5206\u5e03g\u5728\u671d\u5411\u76f8\u673a\u65b9\u5411\u7684\u6807\u51c6\u5dee\u3002</li>\n<li>\u8ba1\u7b97\u5bc6\u5ea6\u503c\uff1a\u6839\u636e\u65b9\u7a0b1\u8ba1\u7b97\u91c7\u6837\u70b9\u7684\u5bc6\u5ea6\u503c\u3002\u5982\u679c\u5b58\u5728\u4e24\u4e2a\u70b9\uff0c\u5176\u5bc6\u5ea6\u503c\u4e00\u4e2a\u4f4e\u4e8e\u03bb\uff0c\u53e6\u4e00\u4e2a\u9ad8\u4e8e\u03bb\uff0c\u5219\u8868\u660e\u5728\u8fd9\u4e24\u4e2a\u70b9\u4e4b\u95f4\u5b58\u5728\u4e00\u4e2a\u6c34\u5e73\u96c6\u70b9\u3002\u8fd9\u65f6\uff0c\u4f7f\u7528\u7ebf\u6027\u63d2\u503c\u6765\u786e\u5b9a\u79bb\u76f8\u673a\u6700\u8fd1\u7684\u6c34\u5e73\u96c6\u70b9\u3002</li>\n<li>\u6cd5\u7ebf\u8ba1\u7b97\uff1a\u5728\u627e\u5230\u7684\u70b9\u5904\u8ba1\u7b97\u8868\u9762\u7684\u6cd5\u7ebf\uff0c\u8fd9\u6709\u52a9\u4e8e\u63d0\u9ad8\u7f51\u683c\u7684\u8d28\u91cf\u3002</li>\n</ol>\n<h3>\n<a name=\"mesh-5\" class=\"anchor\" href=\"#mesh-5\"></a>\u521b\u5efamesh</h3>\n<p>&lt;1&gt;\u7ed1\u5b9a\u65b0\u7684\u9ad8\u65af\u5206\u5e03</p>\n<p>\u4e3a\u4e86\u4f18\u5316\u7f51\u683c\uff0c\u7814\u7a76\u8005\u5c06\u4e00\u7ec4\u65b0\u7684\u9ad8\u65af\u5206\u5e03\u7ed1\u5b9a\u5230\u7f51\u683c\u7684\u4e09\u89d2\u5f62\u4e0a\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4ed6\u4eec\u5c06\u4e00\u7ec4n\u4e2a\u8584\u76843D\u9ad8\u65af\u5206\u5e03\u7ed1\u5b9a\u5230\u7f51\u683c\u4e2d\u7684\u6bcf\u4e2a\u4e09\u89d2\u5f62\u4e0a\u3002\u8fd9\u4e9b\u9ad8\u65af\u5206\u5e03\u662f\u5728\u4e09\u89d2\u5f62\u7684\u8868\u9762\u4e0a\u91c7\u6837\u7684\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u4ed6\u4eec\u7a0d\u5fae\u4fee\u6539\u4e86\u539f\u59cb\u76843D\u9ad8\u65af\u70b9\u4e91\u6a21\u578b\u7684\u7ed3\u6784\uff0c\u663e\u5f0f\u5730\u4ece\u7f51\u683c\u9876\u70b9\u8ba1\u7b97\u9ad8\u65af\u5206\u5e03\u7684\u5747\u503c\uff0c\u4f7f\u7528\u9884\u5b9a\u4e49\u7684\u4e09\u89d2\u5f62\u5185\u7684\u91cd\u5fc3\u5750\u6807\u3002</p>\n<p>&lt;2&gt;\u9ad8\u65af\u5206\u5e03\u7684\u53c2\u6570\u5316</p>\n<p>\u4e3a\u4e86\u4fdd\u6301\u9ad8\u65af\u5206\u5e03\u7684\u5e73\u5766\u6027\u5e76\u4e0e\u7f51\u683c\u4e09\u89d2\u5f62\u5bf9\u9f50\uff0c\u7814\u7a76\u8005\u4f7f\u7528\u4e86\u53ea\u67092\u4e2a\u53ef\u5b66\u4e60\u7684\u7f29\u653e\u56e0\u5b50\u800c\u4e0d\u662f3\u4e2a\uff0c\u4ee5\u53ca\u4ec5\u4f7f\u7528\u4e00\u4e2a\u590d\u6570\u7f16\u7801\u76842D\u65cb\u8f6c\u800c\u4e0d\u662f\u56db\u5143\u6570\u3002\u8fd9\u79cd\u53c2\u6570\u5316\u65b9\u5f0f\u4f7f\u5f97\u9ad8\u65af\u5206\u5e03\u66f4\u52a0\u9002\u5408\u7f51\u683c\u7684\u8868\u9762\uff0c\u5e76\u4e14\u7b80\u5316\u4e86\u4f18\u5316\u8fc7\u7a0b\u3002</p>\n<p>&lt;3&gt;\u4f18\u5316\u8fc7\u7a0b</p>\n<p>\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\uff0c\u7814\u7a76\u8005\u4e0d\u4ec5\u4f18\u5316\u4e86\u9ad8\u65af\u5206\u5e03\u7684\u4f4d\u7f6e\uff0c\u8fd8\u4f18\u5316\u4e86\u6bcf\u4e2a\u9ad8\u65af\u5206\u5e03\u7684\u4e0d\u900f\u660e\u5ea6\u503c\u548c\u4e00\u7ec4\u7403\u8c10\u51fd\u6570\uff0c\u4ee5\u7f16\u7801\u5411\u5404\u4e2a\u65b9\u5411\u53d1\u51fa\u7684\u989c\u8272\u4fe1\u606f\u3002\u8fd9\u4e9b\u4f18\u5316\u786e\u4fdd\u4e86\u5373\u4f7f\u5728\u7f16\u8f91\u7f51\u683c\u4e4b\u540e\uff0c\u6e32\u67d3\u8d28\u91cf\u4f9d\u7136\u5f88\u9ad8\u3002</p>\n<p>&lt;4&gt;\u7f51\u683c\u7f16\u8f91\u7684\u53ef\u80fd\u6027</p>\n<p>\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u7814\u7a76\u8005\u4f7f\u5f97\u7f16\u8f91\u9ad8\u65af\u70b9\u4e91\u573a\u666f\u53d8\u5f97\u66f4\u52a0\u76f4\u89c2\uff0c\u53ef\u4ee5\u4f7f\u7528\u5e38\u89c1\u7684\u7f51\u683c\u7f16\u8f91\u5de5\u5177\u6765\u8fdb\u884c\u7f16\u8f91\uff0c\u5982\u8c03\u6574\u5f62\u72b6\u3001\u6dfb\u52a0\u52a8\u753b\u7b49\uff0c\u540c\u65f6\u4ecd\u7136\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u7684\u6e32\u67d3\u6548\u679c\u3002\u56fe7\u5c55\u793a\u4e86\u5728\u4f18\u5316\u524d\u540e\u7f51\u683c\u7684\u53d8\u5316\uff0c\u800c\u56fe1\u548c\u8865\u5145\u6750\u6599\u5219\u63d0\u4f9b\u4e86\u66f4\u591a\u5173\u4e8e\u901a\u8fc7\u7f16\u8f91\u7f51\u683c\u53ef\u4ee5\u5b9e\u73b0\u7684\u6548\u679c\u7684\u4f8b\u5b50\u3002</p>\n<h2>\n<a name=\"fountain-creative-thinking-6\" class=\"anchor\" href=\"#fountain-creative-thinking-6\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/fountain.png?v=12\" title=\":fountain:\" class=\"emoji\" alt=\":fountain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Creative thinking:</h2>\n<p>TODO\uff1a\u5177\u4f53\u7684\u6548\u679c\u6211\u8fd8\u5f97\u6d4b\u4e00\u4e0b</p>\n<p>\u4f46\u662f\u77e5\u4e4e\u4e0a\u6709\u4e00\u4e2a\u4e0d\u592a\u597d\u7684\u7ed3\u679c\uff1a3D Gaussian Splatting\u80fd\u8f6cMesh\u5417\uff1f - \u6e05\u98ce\u4f3c\u5c11\u5e74\u7684\u56de\u7b54 - \u77e5\u4e4e</p>\n<p><a href=\"https://www.zhihu.com/question/649225673/answer/3464743156\" rel=\"noopener nofollow ugc\">3D Gaussian Splatting\u80fd\u8f6cMesh\u5417\uff1f</a></p>",
        "update": "2024-9-19 15:36:11"
    },
    "9": {
        "name": "Geeve",
        "content": "<h1>\n<a name=\"meteor-melody-aware-texture-controllable-symbolic-orchestral-music-generation-1\" class=\"anchor\" href=\"#meteor-melody-aware-texture-controllable-symbolic-orchestral-music-generation-1\"></a>METEOR: Melody-aware Texture-controllable Symbolic Orchestral Music Generation</h1>\n<ul>\n<li>Conference/Journal: Not specified in the paper, appears to be a preprint</li>\n<li>Authors/group: Dinh-Viet-Toan Le, Yi-Hsuan Yang</li>\n<li>Project page: <a href=\"https://dinhviettoanle.github.io/meteor/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">METEOR: Melody-aware Texture-controllable Symbolic Orchestral Music Generation</a>\n</li>\n</ul>\n<hr>\n<h2>\n<a name=\"spiral_notepad-summary-2\" class=\"anchor\" href=\"#spiral_notepad-summary-2\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/spiral_notepad.png?v=12\" title=\":spiral_notepad:\" class=\"emoji\" alt=\":spiral_notepad:\" loading=\"lazy\" width=\"20\" height=\"20\"> Summary</h2>\n<p>This paper contributes a new model called METEOR for symbolic multi-track music style transfer that focuses on melodic fidelity while allowing control over textural attributes. The model can perform bar-level and track-level controllability of the accompaniment while maintaining a homophonic texture and preserving the melody. It also considers instrumental playability constraints.</p>\n<hr>\n<h2>\n<a name=\"thinking-critical-thinking-3\" class=\"anchor\" href=\"#thinking-critical-thinking-3\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/thinking.png?v=12\" title=\":thinking:\" class=\"emoji\" alt=\":thinking:\" loading=\"lazy\" width=\"20\" height=\"20\"> Critical Thinking</h2>\n<h4>\n<a name=\"sparkles-positive-aspects-4\" class=\"anchor\" href=\"#sparkles-positive-aspects-4\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/sparkles.png?v=12\" title=\":sparkles:\" class=\"emoji\" alt=\":sparkles:\" loading=\"lazy\" width=\"20\" height=\"20\"> Positive Aspects</h4>\n<ul>\n<li>Idea: Combines texture-based style transfer with melodic fidelity, addressing a gap in existing approaches.</li>\n<li>Project: Offers multi-level control (bar-level, track-level) while preserving melody, which is novel.</li>\n<li>Scalability: Can handle different instrument ensembles and adapt to various orchestral configurations.</li>\n<li>Evaluation: Comprehensive evaluation comparing to baselines and analyzing impact of instrumentation and melodic instrument playability.</li>\n</ul>\n<h4>\n<a name=\"cloud_with_rain-negative-aspects-5\" class=\"anchor\" href=\"#cloud_with_rain-negative-aspects-5\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/cloud_with_rain.png?v=12\" title=\":cloud_with_rain:\" class=\"emoji\" alt=\":cloud_with_rain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Negative Aspects</h4>\n<ul>\n<li>System: Slight drop in controllability metrics when adding melodic constraints compared to controllable-only model.</li>\n<li>Design: Compatibility of control levels within latent space or token-based system not fully explored.</li>\n<li>Dataset: Limited information provided about the training dataset (SymphonyNet) and its characteristics.</li>\n</ul>\n<hr>\n<h2>\n<a name=\"fountain-creative-thinking-6\" class=\"anchor\" href=\"#fountain-creative-thinking-6\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/fountain.png?v=12\" title=\":fountain:\" class=\"emoji\" alt=\":fountain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Creative thinking:</h2>\n<p>Some potential extensions or applications of this work could include:</p>\n<ol>\n<li>\n<p>Exploring multi-level controllability including piece-level attributes in addition to bar-level and track-level.</p>\n</li>\n<li>\n<p>Investigating ways to ensure playability of accompaniment parts, not just the melody, considering instrumental constraints and timbre effects.</p>\n</li>\n<li>\n<p>Applying the model to music education, helping students learn orchestration by generating examples with specific textural attributes.</p>\n</li>\n<li>\n<p>Extending the model to handle cross-genre style transfer while maintaining melodic fidelity.</p>\n</li>\n<li>\n<p>Incorporating real-time control capabilities for live performance applications.</p>\n</li>\n<li>\n<p>Exploring the use of this model in film scoring or video game music generation where adaptive orchestration is needed.</p>\n</li>\n<li>\n<p>Investigating how the model could be used for music analysis, automatically identifying and describing textural attributes in existing orchestral works.</p>\n</li>\n</ol>",
        "update": "2024-9-21 9:56:27"
    },
    "10": {
        "name": "Fanseu Kamhoua Barakeel",
        "content": "<h1>\n<a name=\"how-universal-polynomial-bases-enhance-spectral-graph-neural-networks-1\" class=\"anchor\" href=\"#how-universal-polynomial-bases-enhance-spectral-graph-neural-networks-1\"></a>How Universal Polynomial Bases Enhance Spectral Graph Neural Networks:</h1>\n<p>Heterophily, Over-smoothing, and Over-squashing</p>\n<ul>\n<li>Conference/Journal (Name-Year): ICML 2024</li>\n<li>Authors/group: <a href=\"https://openreview.net/profile?id=~Keke_Huang1\" rel=\"noopener nofollow ugc\">Keke Huang</a>, <a href=\"https://openreview.net/profile?id=~Yu_Guang_Wang1\" rel=\"noopener nofollow ugc\">Yu Guang Wang</a>, <a href=\"https://openreview.net/profile?id=~Ming_Li15\" rel=\"noopener nofollow ugc\">Ming Li</a>, <a href=\"https://openreview.net/profile?id=~Pietro_Lio1\" rel=\"noopener nofollow ugc\">Pietro Lio</a>\n</li>\n<li>Github link:</li>\n<li>Project page: <a href=\"https://openreview.net/forum?id=Z2LH6Va7L2\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">How Universal Polynomial Bases Enhance Spectral Graph Neural Networks: Heterophily, Over-smoothing, and Over-squashing | OpenReview</a>\n</li>\n</ul>\n<p><img src=\"//bbs.hkustvis.org/uploads/default/original/2X/2/256cd547cd593653386a0db79c4ecc80e096ac15.png\" alt=\"image\" data-base62-sha1=\"5l4Q68kUEJfOgCmK8iP7EDD5Keh\" width=\"460\" height=\"319\"></p>\n<p><img src=\"//bbs.hkustvis.org/uploads/default/original/2X/1/117504b303e0da4bac675420e7ca6b53fef42ced.png\" alt=\"image\" data-base62-sha1=\"2uqPjz0fxpfpj7gBzwQGJ93g4xf\" width=\"459\" height=\"204\"></p>\n<hr>\n<h2>\n<a name=\"spiral_notepad-summary-2\" class=\"anchor\" href=\"#spiral_notepad-summary-2\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/spiral_notepad.png?v=12\" title=\":spiral_notepad:\" class=\"emoji\" alt=\":spiral_notepad:\" loading=\"lazy\" width=\"20\" height=\"20\"> Summary</h2>\n<p>This paper studies the usage of polynomial filters to address a number of problems in Graph learning. Mainly they study the heterophily problem in depth, and propose a theorem prooving that the ideal bases needs higher order signals which are often ignored by most proposed filters till date\u2026</p>\n<hr>\n<h2>\n<a name=\"thinking-critical-thinking-3\" class=\"anchor\" href=\"#thinking-critical-thinking-3\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/thinking.png?v=12\" title=\":thinking:\" class=\"emoji\" alt=\":thinking:\" loading=\"lazy\" width=\"20\" height=\"20\"> Critical Thinking</h2>\n<h4>\n<a name=\"sparkles-positive-aspects-4\" class=\"anchor\" href=\"#sparkles-positive-aspects-4\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/sparkles.png?v=12\" title=\":sparkles:\" class=\"emoji\" alt=\":sparkles:\" loading=\"lazy\" width=\"20\" height=\"20\"> Positive Aspects</h4>\n<ul>\n<li>Idea: Great idea pointing out the problem with hand crafted filters. Why can\u2019t we learn the filters is an idea to explore after CVPR</li>\n<li>Project</li>\n<li>Scalability</li>\n<li>Evaluation</li>\n</ul>\n<h4>\n<a name=\"cloud_with_rain-negative-aspects-5\" class=\"anchor\" href=\"#cloud_with_rain-negative-aspects-5\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/cloud_with_rain.png?v=12\" title=\":cloud_with_rain:\" class=\"emoji\" alt=\":cloud_with_rain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Negative Aspects</h4>\n<ul>\n<li>System: polynomial filters are not very new, and are also hand crafted</li>\n<li>Design</li>\n<li>Dataset</li>\n</ul>\n<hr>\n<h2>\n<a name=\"fountain-creative-thinking-6\" class=\"anchor\" href=\"#fountain-creative-thinking-6\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/fountain.png?v=12\" title=\":fountain:\" class=\"emoji\" alt=\":fountain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Creative thinking:</h2>",
        "update": "2024-9-22 13:48:24"
    },
    "16": {
        "name": "Jason Wong",
        "content": "<h1>\n<a name=\"exploring-the-design-of-physical-artefacts-to-visualise-household-consumption-for-encouraging-sustainable-practices-1\" class=\"anchor\" href=\"#exploring-the-design-of-physical-artefacts-to-visualise-household-consumption-for-encouraging-sustainable-practices-1\"></a>Exploring the design of physical artefacts to visualise household consumption for encouraging sustainable practices</h1>\n<ul>\n<li>Conference/Journal (Name-Year): Behavior and Information Technology 2024</li>\n</ul>\n<hr>\n<h2>\n<a name=\"summary-2\" class=\"anchor\" href=\"#summary-2\"></a>Summary</h2>\n<p>Household consumption significantly contributes to climate change, but understanding its impact on a personal level can be challenging. This paper investigates the use of physical data visualizations to encourage sustainable behaviors in households. Through 15 design workshops with household participants, the authors explored physical designs and imagined how tangible artefacts might influence sustainability practices using five prototypes. The results emphasize the importance of balancing aesthetics, abstraction, and actionable insights in the design of these artefacts. Three key considerations were distilled: (1) involving all household members in the design process to reflect shared values and foster engagement, (2) incorporating child-friendly elements to promote sustainability awareness and education in children, and (3) using nature-inspired visuals to alleviate information overload and simplify understanding.</p>\n<hr>\n<h2>\n<a name=\"positive-3\" class=\"anchor\" href=\"#positive-3\"></a>Positive</h2>\n<ol>\n<li>Held design workshops by the people for the people.</li>\n<li>Interesting to see the different demands among multiple- and single-occupant homes.</li>\n</ol>\n<h2>\n<a name=\"negative-4\" class=\"anchor\" href=\"#negative-4\"></a>Negative</h2>\n<ol>\n<li>The considerations for developing these prototypes were missing. There are no discussions for why they were developed, what had inspired and motivated the design, and followed what design principles.</li>\n<li>Physical artefacts at homes, from everyone\u2019s daily experience, are easily ignored after the sense of freshness is gone. The design workshops primarily focus on short-term effects, without prolonged impact on the lifestyle, which is essential for sustainability.</li>\n</ol>\n<h2>\n<a name=\"my-approach-to-the-problem-5\" class=\"anchor\" href=\"#my-approach-to-the-problem-5\"></a>My approach to the problem</h2>\n<ol>\n<li>I would think of metrics to rank the five prototypes.</li>\n</ol>\n<h2>\n<a name=\"implication-to-my-research-6\" class=\"anchor\" href=\"#implication-to-my-research-6\"></a>Implication to my research</h2>\n<ol>\n<li>I have not read the paper from 2008 that describe different types of visualization. The embodiment concept in that paper is still applicable to today\u2019s research.\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"//bbs.hkustvis.org/uploads/default/original/2X/b/b95e894e8ed63ba85a2a7cca9a1b8b0b0ef8c7b2.jpeg\" data-download-href=\"//bbs.hkustvis.org/uploads/default/b95e894e8ed63ba85a2a7cca9a1b8b0b0ef8c7b2\" title=\"image\"><img src=\"//bbs.hkustvis.org/uploads/default/optimized/2X/b/b95e894e8ed63ba85a2a7cca9a1b8b0b0ef8c7b2_2_187x249.jpeg\" alt=\"image\" data-base62-sha1=\"qrQSgq3XHk8xYQnUwTXrDPgddVU\" width=\"187\" height=\"249\" srcset=\"//bbs.hkustvis.org/uploads/default/optimized/2X/b/b95e894e8ed63ba85a2a7cca9a1b8b0b0ef8c7b2_2_187x249.jpeg, //bbs.hkustvis.org/uploads/default/optimized/2X/b/b95e894e8ed63ba85a2a7cca9a1b8b0b0ef8c7b2_2_280x373.jpeg 1.5x, //bbs.hkustvis.org/uploads/default/optimized/2X/b/b95e894e8ed63ba85a2a7cca9a1b8b0b0ef8c7b2_2_374x498.jpeg 2x\" data-dominant-color=\"F0F0EF\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1332\u00d71774 126 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div>\n</li>\n<li>\u201cHowever, a challenge persists in the landscape of data physicalization design: the limited capacity of existing designs to foster collaborative discussions and interpretation encompassing all family members that would seamlessly integrate into the home environment (Perera et al. 2023b).\u201d There are many many papers these days starting to discuss family-based collaborations. This is a blue ocean that has tremendous growth potential.</li>\n<li>Is workshop-based methodology a type of User-Generated Content? How can we collect and provide a platform for these research?</li>\n<li>\u201cUsing understandable visuals (Bae et al. 2023), and / or age-appropriate language could ensure children can comprehend the information presented\u201d What does the age-appropriate language look like for data visualizations?</li>\n</ol>",
        "update": "2024-9-22 11:21:46"
    },
    "17": {
        "name": "Shelly",
        "content": "<p>Reviewed a TVCG paper.</p>",
        "update": "2024-9-20 18:12:27"
    },
    "20": {
        "name": "Yifan Cao",
        "content": "<h1>\n<a name=\"sensecape-enabling-multilevel-exploration-and-sensemaking-with-large-language-models-1\" class=\"anchor\" href=\"#sensecape-enabling-multilevel-exploration-and-sensemaking-with-large-language-models-1\"></a>Sensecape: Enabling Multilevel Exploration and Sensemaking with Large Language Models</h1>\n<ul>\n<li>Conference/Journal (Name-Year): UIST 2024</li>\n<li>Authors/group: Sangho Suh, Bryan Min, Srishti Palani, and Haijun Xia, the University of California, San Diego.</li>\n<li>Github link: <a href=\"https://sensecape.github.io/\" rel=\"noopener nofollow ugc\">https://sensecape.github.io/</a>\n</li>\n</ul>\n<hr>\n<h2>\n<a name=\"spiral_notepad-summary-2\" class=\"anchor\" href=\"#spiral_notepad-summary-2\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/spiral_notepad.png?v=12\" title=\":spiral_notepad:\" class=\"emoji\" alt=\":spiral_notepad:\" loading=\"lazy\" width=\"20\" height=\"20\"> Summary</h2>\n<p>Researchers have observed that people are increasingly turning to large language models (LLMs) for complex information tasks such as academic research or planning a move to another city. However, these tasks often require working in a nonlinear manner\u2014for example, arranging information spatially to organize and make sense of it\u2014while current interfaces for interacting with LLMs are generally linear to support conversational interaction.</p>\n<p>To address this limitation and explore how to support LLM-powered exploration and sensemaking, the research team developed Sensecape, an interactive system designed to support complex information tasks with an LLM. Sensecape enables users to (1) manage the complexity of information through multilevel abstraction and (2) switch seamlessly between foraging and sensemaking.</p>\n<p>Their within-subject user study reveals that Sensecape empowers users to explore more topics and structure their knowledge hierarchically, thanks to the externalization of levels of abstraction. This research contributes implications for LLM-based workflows and interfaces for information tasks.</p>\n<hr>\n<h2>\n<a name=\"thinking-critical-thinking-3\" class=\"anchor\" href=\"#thinking-critical-thinking-3\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/thinking.png?v=12\" title=\":thinking:\" class=\"emoji\" alt=\":thinking:\" loading=\"lazy\" width=\"20\" height=\"20\"> Critical Thinking</h2>\n<h4>\n<a name=\"sparkles-positive-aspects-4\" class=\"anchor\" href=\"#sparkles-positive-aspects-4\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/sparkles.png?v=12\" title=\":sparkles:\" class=\"emoji\" alt=\":sparkles:\" loading=\"lazy\" width=\"20\" height=\"20\"> Positive Aspects</h4>\n<p><strong>Research Background:</strong></p>\n<p>\u2022 People are increasingly using large language models (LLMs) for complex information tasks</p>\n<p>\u2022 Current LLM interfaces are linear and conversational, which limits their effectiveness for nonlinear tasks</p>\n<p><strong>Research Motivation:</strong></p>\n<p>\u2022 To reconcile the mismatch between linear LLM interfaces and nonlinear complex information tasks</p>\n<p>\u2022 To enable fluid exploration and sensemaking workflows with LLMs</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"//bbs.hkustvis.org/uploads/default/original/2X/1/1aed84f1b3dd9f43e1bc4c26f58fce31ea2889ab.jpeg\" data-download-href=\"//bbs.hkustvis.org/uploads/default/1aed84f1b3dd9f43e1bc4c26f58fce31ea2889ab\" title=\"image\"><img src=\"//bbs.hkustvis.org/uploads/default/optimized/2X/1/1aed84f1b3dd9f43e1bc4c26f58fce31ea2889ab_2_690x297.jpeg\" alt=\"image\" data-base62-sha1=\"3QdipIwgY2QkQ4CxvfhfWVu9YSL\" width=\"690\" height=\"297\" srcset=\"//bbs.hkustvis.org/uploads/default/optimized/2X/1/1aed84f1b3dd9f43e1bc4c26f58fce31ea2889ab_2_690x297.jpeg, //bbs.hkustvis.org/uploads/default/optimized/2X/1/1aed84f1b3dd9f43e1bc4c26f58fce31ea2889ab_2_1035x445.jpeg 1.5x, //bbs.hkustvis.org/uploads/default/optimized/2X/1/1aed84f1b3dd9f43e1bc4c26f58fce31ea2889ab_2_1380x594.jpeg 2x\" data-dominant-color=\"E4E2E3\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1890\u00d7814 139 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><strong>Challenges:</strong></p>\n<ul>\n<li>\n<p><strong>Slow Start:</strong> Users struggle to determine where to start and what questions to ask when they have limited knowledge of a complex topic.</p>\n</li>\n<li>\n<p><strong>Hard to Revisit</strong>: Linear interfaces make it difficult for users to navigate back to previous information.</p>\n</li>\n<li>\n<p><strong>Lack of Structure</strong>: The inability to group and connect information makes sensemaking difficult.</p>\n</li>\n<li>\n<p><strong>Information Overload</strong>: LLMs can generate overwhelming amounts of text, making it difficult for users to process information.</p>\n</li>\n<li>\n<p><strong>Visual Clutter</strong>: Multiple topics explored on a single canvas can lead to visual clutter.</p>\n</li>\n<li>\n<p><strong>Cost of Context-Switching</strong>: Users are forced to constantly switch between different tools for information exploration and sensemaking.</p>\n</li>\n</ul>\n<p><strong>Key Contributions:</strong></p>\n<p>\u2022 Sensecape: An interactive system that enables multilevel exploration and sensemaking with LLMs</p>\n<p>\u2022 Externalization of multilevel abstraction for more comprehensive and effective exploration</p>\n<p>\u2022 A user study demonstrating the benefits of seamless exploration across semantic levels</p>\n<p>\u2022 Implications for designing LLM-based workflows and interfaces for complex information tasks</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"//bbs.hkustvis.org/uploads/default/original/2X/b/b50f6b8ca37b2405ffefe6915ed6b04017f63a87.jpeg\" data-download-href=\"//bbs.hkustvis.org/uploads/default/b50f6b8ca37b2405ffefe6915ed6b04017f63a87\" title=\"image\"><img src=\"//bbs.hkustvis.org/uploads/default/optimized/2X/b/b50f6b8ca37b2405ffefe6915ed6b04017f63a87_2_690x355.jpeg\" alt=\"image\" data-base62-sha1=\"pPJsuGhQuocLbLuncymWsoQ3OYf\" width=\"690\" height=\"355\" srcset=\"//bbs.hkustvis.org/uploads/default/optimized/2X/b/b50f6b8ca37b2405ffefe6915ed6b04017f63a87_2_690x355.jpeg, //bbs.hkustvis.org/uploads/default/optimized/2X/b/b50f6b8ca37b2405ffefe6915ed6b04017f63a87_2_1035x532.jpeg 1.5x, //bbs.hkustvis.org/uploads/default/optimized/2X/b/b50f6b8ca37b2405ffefe6915ed6b04017f63a87_2_1380x710.jpeg 2x\" data-dominant-color=\"DFE4EA\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1920\u00d7990 215 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><strong>The system addresses these challenges by:</strong></p>\n<p>\u2022 Providing a nonlinear interface suitable for exploratory tasks</p>\n<p>\u2022 Enabling flexible navigation between different levels of abstraction in the information space</p>\n<p>\u2022 Allowing users to switch between canvas and hierarchy views for multilevel exploration and sensemaking</p>\n<p>\u2022 Externalizing the concept of levels of abstraction to provide a comprehensive overview of the information space</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"//bbs.hkustvis.org/uploads/default/original/2X/4/4bcdac609d7eb28506973e54556430a6ed81031d.jpeg\" data-download-href=\"//bbs.hkustvis.org/uploads/default/4bcdac609d7eb28506973e54556430a6ed81031d\" title=\"image\"><img src=\"//bbs.hkustvis.org/uploads/default/optimized/2X/4/4bcdac609d7eb28506973e54556430a6ed81031d_2_690x396.jpeg\" alt=\"image\" data-base62-sha1=\"aOAsJjHFza8PkmpwDinNgwWhLjL\" width=\"690\" height=\"396\" srcset=\"//bbs.hkustvis.org/uploads/default/optimized/2X/4/4bcdac609d7eb28506973e54556430a6ed81031d_2_690x396.jpeg, //bbs.hkustvis.org/uploads/default/optimized/2X/4/4bcdac609d7eb28506973e54556430a6ed81031d_2_1035x594.jpeg 1.5x, //bbs.hkustvis.org/uploads/default/optimized/2X/4/4bcdac609d7eb28506973e54556430a6ed81031d_2_1380x792.jpeg 2x\" data-dominant-color=\"EEEFEF\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1918\u00d71102 143 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<h4>\n<a name=\"cloud_with_rain-negative-aspects-5\" class=\"anchor\" href=\"#cloud_with_rain-negative-aspects-5\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/cloud_with_rain.png?v=12\" title=\":cloud_with_rain:\" class=\"emoji\" alt=\":cloud_with_rain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Negative Aspects</h4>\n<p>First, the study\u2019s sample size is relatively small, with only 12 participants. This limits the generalizability of the findings, and it would be important to see how these results hold up with a larger and more diverse group of users. Additionally, the participants were recruited from a single university, which could bias the results. A more diverse sample representing a wider range of users would be more representative.</p>\n<p>Second, the study\u2019s duration was limited to 20 minutes per task. This might not have been sufficient time for participants to fully understand and leverage all of Sensecape\u2019s features, especially the hierarchy view, which some participants found complex and overwhelming. A longitudinal study spanning several weeks or months in a real-world setting would provide more valuable insights into the long-term usability and effectiveness of the system.</p>\n<p>Finally, while the research acknowledges the limitations of LLMs, it primarily focuses on supporting information exploration and sensemaking without delving into the inherent challenges of trust and accuracy. As LLMs become increasingly powerful, it\u2019s crucial to address the issue of \u201challucination\u201d by implementing mechanisms to verify the accuracy of the generated information.</p>\n<hr>\n<h2>\n<a name=\"fountain-creative-thinking-6\" class=\"anchor\" href=\"#fountain-creative-thinking-6\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/fountain.png?v=12\" title=\":fountain:\" class=\"emoji\" alt=\":fountain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Creative thinking:</h2>\n<p>Address \u201cHallucination\u201d in LLMs:  The research acknowledges the limitations of LLMs, particularly the issue of \u201challucination,\u201d where they generate inaccurate or fabricated information. You could delve deeper into research related to fact-checking and trust-building mechanisms for LLM-powered systems. This could involve investigating how to integrate reliable sources, provide evidence trails, or develop user interfaces that promote critical evaluation of LLM-generated information.</p>",
        "update": "2024-9-22 12:15:46"
    },
    "22": {
        "name": "SHENG Rui",
        "content": "<h1>\n<a name=\"choicemates-supporting-unfamiliar-online-decision-making-with-multi-agent-conversational-interactions-1\" class=\"anchor\" href=\"#choicemates-supporting-unfamiliar-online-decision-making-with-multi-agent-conversational-interactions-1\"></a>ChoiceMates: Supporting Unfamiliar Online Decision-Making with Multi Agent Conversational Interactions</h1>\n<ul>\n<li>Conference/Journal (Name-Year): arxiv - 2023</li>\n<li>Authors/group: KAIST - Jeongeon Park, Bryan Min, Xiaojuan Ma, Juho Kim</li>\n<li>Project page: <a href=\"https://arxiv.org/pdf/2310.01331\">https://arxiv.org/pdf/2310.01331</a>\n</li>\n</ul>\n<hr>\n<h2>\n<a name=\"spiral_notepad-summary-2\" class=\"anchor\" href=\"#spiral_notepad-summary-2\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/spiral_notepad.png?v=12\" title=\":spiral_notepad:\" class=\"emoji\" alt=\":spiral_notepad:\" loading=\"lazy\" width=\"20\" height=\"20\"> Summary</h2>\n<p>This paper introduced how to leverage multi-agents to help users address decision-making in unfamiliar scenarios. In such scenarios, users face several challenges: how to find diverse perspectives, how to seek relevant information, and when to stop. This paper proposed to make each agent play a different user with different criteria and options. Then end users can interact with those agents to get more tailored decision information.\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"//bbs.hkustvis.org/uploads/default/original/2X/8/88672395c9b6de5545ff9cee3b48a3e853144389.jpeg\" data-download-href=\"//bbs.hkustvis.org/uploads/default/88672395c9b6de5545ff9cee3b48a3e853144389\" title=\"image\"><img src=\"//bbs.hkustvis.org/uploads/default/optimized/2X/8/88672395c9b6de5545ff9cee3b48a3e853144389_2_690x261.jpeg\" alt=\"image\" data-base62-sha1=\"jsFUr6Uo8Bq7U0BrN63LVNoRWpr\" width=\"690\" height=\"261\" srcset=\"//bbs.hkustvis.org/uploads/default/optimized/2X/8/88672395c9b6de5545ff9cee3b48a3e853144389_2_690x261.jpeg, //bbs.hkustvis.org/uploads/default/optimized/2X/8/88672395c9b6de5545ff9cee3b48a3e853144389_2_1035x391.jpeg 1.5x, //bbs.hkustvis.org/uploads/default/optimized/2X/8/88672395c9b6de5545ff9cee3b48a3e853144389_2_1380x522.jpeg 2x\" data-dominant-color=\"F4F5F6\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1614\u00d7611 108 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<hr>\n<h2>\n<a name=\"thinking-critical-thinking-3\" class=\"anchor\" href=\"#thinking-critical-thinking-3\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/thinking.png?v=12\" title=\":thinking:\" class=\"emoji\" alt=\":thinking:\" loading=\"lazy\" width=\"20\" height=\"20\"> Critical Thinking</h2>\n<h4>\n<a name=\"sparkles-positive-aspects-4\" class=\"anchor\" href=\"#sparkles-positive-aspects-4\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/sparkles.png?v=12\" title=\":sparkles:\" class=\"emoji\" alt=\":sparkles:\" loading=\"lazy\" width=\"20\" height=\"20\"> Positive Aspects</h4>\n<ul>\n<li>I think the scenario is very interesting. Unfamiliar decision-making scenarios are totally different from cases where decision-makers have domain knowledge.</li>\n<li>The formative study is solid. There are 14 participants covering different decision-making scenarios.</li>\n<li>The presentation is great, with clear writing and figures.</li>\n</ul>\n<h4>\n<a name=\"cloud_with_rain-negative-aspects-5\" class=\"anchor\" href=\"#cloud_with_rain-negative-aspects-5\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/cloud_with_rain.png?v=12\" title=\":cloud_with_rain:\" class=\"emoji\" alt=\":cloud_with_rain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Negative Aspects</h4>\n<ul>\n<li>I don\u2019t understand why they need to use multi-agents. First of all, why can\u2019t we just summarize all the information for the user? For example, list all the options. If we only show a few agents, is this sure not to introduce bias?</li>\n<li>Can the selection of different criteria really be determined solely by these keywords? For example, image quality. What kind of image quality do users want? If I am a user without any domain knowledge, it is difficult for me to consider whether I need image quality or not.</li>\n<li>I think a more appropriate approach is to implement a case-based approach. For image quality, ease of use, and other criteria, users need to have a more intuitive understanding of how to set the criteria.</li>\n<li>I doubt the completeness of the post information. If the post does not explicitly state that every product is of good quality, does that necessarily mean that the product quality is poor?</li>\n<li>How to address criteria conflict for users? For example, they may want to choose a very light camera but also a camera with high image quality. But what if there is no such camera? This kind of problem often occurs in multi-criteria decision-making.</li>\n<li>Writing issues: Don\u2019t explicitly mention how to address the third challenge. And use C to represents challenge and participants, which is very confusing.</li>\n</ul>\n<hr>\n<h2>\n<a name=\"fountain-creative-thinking-6\" class=\"anchor\" href=\"#fountain-creative-thinking-6\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/fountain.png?v=12\" title=\":fountain:\" class=\"emoji\" alt=\":fountain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Creative thinking:</h2>\n<ul>\n<li>How users decide criteria is very challenging. How do users perceive criteria when they are unfamiliar with a decision-making scenario? For example, what is ease of use?</li>\n<li>How can decision-makers compare different options considering the conflict between multiple criteria in this situation?</li>\n</ul>",
        "update": "2024-9-22 8:14:18"
    },
    "24": {
        "name": "CHEN, Zixin Steven",
        "content": "<h1>\n<a name=\"readingqizmaker-a-human-nlp-collaborative-system-that-supports-instructors-to-design-high-qality-reading-qiz-qestions-1\" class=\"anchor\" href=\"#readingqizmaker-a-human-nlp-collaborative-system-that-supports-instructors-to-design-high-qality-reading-qiz-qestions-1\"></a>ReadingQizMaker: A Human-NLP Collaborative System that Supports Instructors to Design High-Qality Reading Qiz Qestions</h1>\n<ul>\n<li>Conference/Journal (Name-Year): CHI 2023 Honorable Mention</li>\n<li>Authors/group: Xinyi Lu</li>\n<li>Github link:</li>\n<li>Project page:</li>\n</ul>\n<hr>\n<h2>\n<a name=\"spiral_notepad-summary-2\" class=\"anchor\" href=\"#spiral_notepad-summary-2\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/spiral_notepad.png?v=12\" title=\":spiral_notepad:\" class=\"emoji\" alt=\":spiral_notepad:\" loading=\"lazy\" width=\"20\" height=\"20\"> Summary</h2>\n<p>This paper contributes in designing and developing \u201cReadingQuizMaker\u201d, a human-NLP collaborative system designed to assist instructors in creating high-quality reading quiz questions. It adapts to the natural workflow of instructors, offering NLP-based support such as paraphrasing, summarization, and negation to enhance question creation. Instructors can maintain control over the question design process while benefiting from AI suggestions. The system was evaluated through user studies, and the results showed that instructors found it time-saving and useful, particularly for generating distractors and question stems. The study concluded that instructors prefer human-AI collaboration over fully automated question generation, as it provides more control and creativity.</p>\n<hr>\n<h2>\n<a name=\"thinking-critical-thinking-3\" class=\"anchor\" href=\"#thinking-critical-thinking-3\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/thinking.png?v=12\" title=\":thinking:\" class=\"emoji\" alt=\":thinking:\" loading=\"lazy\" width=\"20\" height=\"20\"> Critical Thinking</h2>\n<h4>\n<a name=\"sparkles-positive-aspects-4\" class=\"anchor\" href=\"#sparkles-positive-aspects-4\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/sparkles.png?v=12\" title=\":sparkles:\" class=\"emoji\" alt=\":sparkles:\" loading=\"lazy\" width=\"20\" height=\"20\"> Positive Aspects</h4>\n<ul>\n<li>\n<p>Idea:</p>\n<ul>\n<li>\n<p>This idea is definitely an important topic in the education domain. Quiz questions generated through reading materials is a common, significant, and also time-consuming tasks for instructors.</p>\n</li>\n<li>\n<p><strong>Enhanced Efficiency</strong>: Instructors found that using the system saved time in generating quiz questions, especially for challenging tasks like creating distractors.</p>\n</li>\n<li>\n<p><strong>AI Assistance with Control</strong>: Instructors appreciated that they had control over the AI suggestions, allowing them to adopt or modify results based on their expertise.</p>\n</li>\n<li>\n<p><strong>Effective Collaboration</strong>: The human-AI collaborative model was favored over automatic question generation, as it maintained higher question quality and aligned with educational goals.</p>\n</li>\n</ul>\n</li>\n</ul>\n<h4>\n<a name=\"cloud_with_rain-negative-aspects-5\" class=\"anchor\" href=\"#cloud_with_rain-negative-aspects-5\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/cloud_with_rain.png?v=12\" title=\":cloud_with_rain:\" class=\"emoji\" alt=\":cloud_with_rain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Negative Aspects</h4>\n<ul>\n<li>\n<strong>AI Limitations</strong>: After the birth of ChatGPT, the previous generation of bert-based NLP models are out-of-the-stage. Meanwhile, some AI-generated suggestions, particularly for paraphrasing and distractors, required further modification by instructors, indicating that the AI results were not always accurate or fully usable.</li>\n<li>\n<strong>Domain-Specific</strong>: The system works better with academic papers but struggles with non-academic sources like tutorials or news articles, limiting its versatility.</li>\n<li>\n<strong>Complexity for New Users</strong>: While the system was generally user-friendly, new users needed some time to familiarize themselves with its features, especially NLP transformations.</li>\n</ul>\n<hr>\n<h2>\n<a name=\"fountain-creative-thinking-6\" class=\"anchor\" href=\"#fountain-creative-thinking-6\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/fountain.png?v=12\" title=\":fountain:\" class=\"emoji\" alt=\":fountain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Creative thinking:</h2>\n<p>The idea of combining human expertise with AI suggestions is a significant advancement for instructional design. The system\u2019s ability to provide immediate AI suggestions like paraphrasing, summarization, and negation opens up new possibilities for automating repetitive tasks while leaving complex decisions to human instructors. A potential improvement could involve refining the AI to better handle non-academic content or integrating adaptive learning features that suggest different strategies based on the content type or student performance data. Another future direction could involve expanding the system\u2019s feedback mechanism, enabling real-time collaboration between instructors and students during the question design and answering process.</p>",
        "update": "2024-9-21 6:57:51"
    },
    "25": {
        "name": "Kentaro Takahira",
        "content": "<p>I was focusing on the submission until Friday this week. I proofread papers from our lab for pacific vis and gave reviews.</p>",
        "update": "2024-9-22 12:46:53"
    },
    "26": {
        "name": "Kento Shigyo",
        "content": "<p>I\u2019m working on my submission for CSCW.</p>",
        "update": "2024-9-22 11:29:16"
    },
    "28": {
        "name": "Leixian Shen",
        "content": "<p>Busy for PQE and revise the CHI submission into PQE report, post in the PQE progress page</p>",
        "update": "2024-9-21 13:47:35"
    },
    "29": {
        "name": "Zhonghua",
        "content": "<h1>\n<a name=\"worldscribe-towards-context-aware-live-visual-descriptions-1\" class=\"anchor\" href=\"#worldscribe-towards-context-aware-live-visual-descriptions-1\"></a>WorldScribe: Towards Context-Aware Live Visual Descriptions</h1>\n<ul>\n<li>Conference/Journal (Name-Year): UIST\u201924 (best paper)</li>\n<li>Authors/group:Ruei-Che Chang, Yuxuan Liu, Anhong Guo</li>\n</ul>\n<hr>\n<h2>\n<a name=\"spiral_notepad-summary-2\" class=\"anchor\" href=\"#spiral_notepad-summary-2\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/spiral_notepad.png?v=12\" title=\":spiral_notepad:\" class=\"emoji\" alt=\":spiral_notepad:\" loading=\"lazy\" width=\"20\" height=\"20\"> Summary</h2>\n<p>WorldScribe is a system that generates automated live real-world visual descriptions that are customizable and adaptive to users\u2019 contexts. It provides descriptions tailored to users\u2019 intents and prioritized based on semantic relevance. WorldScribe is adaptive to visual contexts, providing succinct descriptions for dynamic scenes and detailed ones for stable settings. It is also adaptive to sound contexts, adjusting volume in noisy environments or pausing when conversations start. Powered by vision, language, and sound recognition models, WorldScribe balances the tradeoffs between description richness and latency to support real-time use. The system was informed by prior work and a formative study with blind participants, and its user study and pipeline evaluation show it can provide real-time and accurate visual descriptions that are adaptive and customized to users\u2019 contexts.</p>\n<hr>\n<h2>\n<a name=\"thinking-critical-thinking-3\" class=\"anchor\" href=\"#thinking-critical-thinking-3\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/thinking.png?v=12\" title=\":thinking:\" class=\"emoji\" alt=\":thinking:\" loading=\"lazy\" width=\"20\" height=\"20\"> Critical Thinking</h2>\n<h4>\n<a name=\"sparkles-positive-aspects-4\" class=\"anchor\" href=\"#sparkles-positive-aspects-4\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/sparkles.png?v=12\" title=\":sparkles:\" class=\"emoji\" alt=\":sparkles:\" loading=\"lazy\" width=\"20\" height=\"20\"> Positive Aspects</h4>\n<ul>\n<li>Idea related to blind people always draws great attention</li>\n<li>The design and argumentation process is very detailed and rigorous.</li>\n<li>The pipeline is illustrated very clearly</li>\n</ul>\n<h4>\n<a name=\"cloud_with_rain-negative-aspects-5\" class=\"anchor\" href=\"#cloud_with_rain-negative-aspects-5\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/cloud_with_rain.png?v=12\" title=\":cloud_with_rain:\" class=\"emoji\" alt=\":cloud_with_rain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Negative Aspects</h4>\n<ul>\n<li>No \u2018Tech\u2019, all are using existing techs (This is also a good aspect. Old Tech, new usage)</li>\n</ul>\n<hr>\n<h2>\n<a name=\"fountain-creative-thinking-6\" class=\"anchor\" href=\"#fountain-creative-thinking-6\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/fountain.png?v=12\" title=\":fountain:\" class=\"emoji\" alt=\":fountain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Creative thinking:</h2>\n<p>My idea of the environment risk detection for children? Also able to make a paper in this way?</p>",
        "update": "2024-9-22 3:0:8"
    },
    "30": {
        "name": "Yihao Meng",
        "content": "<h1>\n<a name=\"tooncrafter-generative-cartoon-interpolation-1\" class=\"anchor\" href=\"#tooncrafter-generative-cartoon-interpolation-1\"></a>ToonCrafter: Generative Cartoon Interpolation</h1>\n<ul>\n<li>Conference/Journal (Name-Year): <strong>ACM Transactions on Graphics (Special issue of SIGGRAPH Asia 2024)</strong>\n</li>\n<li>Authors/group: <a href=\"https://doubiiu.github.io/\" rel=\"noopener nofollow ugc\">Jinbo Xing</a>1, <a href=\"https://github.com/hyliu\" rel=\"noopener nofollow ugc\">Hanyuan Liu</a>2, <a href=\"https://menghanxia.github.io/\" rel=\"noopener nofollow ugc\">Menghan Xia</a>3, <a href=\"https://yzhang2016.github.io/\" rel=\"noopener nofollow ugc\">Yong Zhang</a>3, <a href=\"https://xinntao.github.io/\" rel=\"noopener nofollow ugc\">Xintao Wang</a>3, <a href=\"https://scholar.google.com/citations?hl=en&amp;user=4oXBp9UAAAAJ&amp;view_op=list_works&amp;sortby=pubdate\" rel=\"noopener nofollow ugc\">Ying Shan</a>3, <a href=\"https://www.cse.cuhk.edu.hk/~ttwong/myself.html\" rel=\"noopener nofollow ugc\">Tien-Tsin Wong</a>1,4</li>\n</ul>\n<p>1The Chinese University of Hong Kong, 2City University of Hong Kong, 3Tencent AI Lab, 4Monash University</p>\n<ul>\n<li>Github link: <a href=\"https://github.com/Doubiiu/ToonCrafter\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - Doubiiu/ToonCrafter: [SIGGRAPH Asia 2024, Journal Track] ToonCrafter: Generative Cartoon Interpolation</a>\n</li>\n<li>Project page: <a href=\"https://doubiiu.github.io/projects/ToonCrafter/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">ToonCrafter: Generative Cartoon Interpolation</a>\n</li>\n</ul>\n<hr>\n<h2>\n<a name=\"spiral_notepad-summary-2\" class=\"anchor\" href=\"#spiral_notepad-summary-2\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/spiral_notepad.png?v=12\" title=\":spiral_notepad:\" class=\"emoji\" alt=\":spiral_notepad:\" loading=\"lazy\" width=\"20\" height=\"20\"> Summary</h2>\n<p>This paper focus on finetuning video generation model to cartoon generation task and achieve very impressive results. The author collects dataset by preprocess classical cartoon movie. The model also support sparse sketch as condition, which aligns well with artists\u2019 process of creating cartoon.\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"//bbs.hkustvis.org/uploads/default/original/2X/c/c91363f1ba6eda6db8ae1ee332e0c4f35262b3c1.jpeg\" data-download-href=\"//bbs.hkustvis.org/uploads/default/c91363f1ba6eda6db8ae1ee332e0c4f35262b3c1\" title=\"\u622a\u5c4f2024-09-22 19.38.29\"><img src=\"//bbs.hkustvis.org/uploads/default/optimized/2X/c/c91363f1ba6eda6db8ae1ee332e0c4f35262b3c1_2_690x434.jpeg\" alt=\"\u622a\u5c4f2024-09-22 19.38.29\" data-base62-sha1=\"sGNvWSgCHSTtKdMeEmnQiZBBLwZ\" width=\"690\" height=\"434\" srcset=\"//bbs.hkustvis.org/uploads/default/optimized/2X/c/c91363f1ba6eda6db8ae1ee332e0c4f35262b3c1_2_690x434.jpeg, //bbs.hkustvis.org/uploads/default/optimized/2X/c/c91363f1ba6eda6db8ae1ee332e0c4f35262b3c1_2_1035x651.jpeg 1.5x, //bbs.hkustvis.org/uploads/default/optimized/2X/c/c91363f1ba6eda6db8ae1ee332e0c4f35262b3c1_2_1380x868.jpeg 2x\" data-dominant-color=\"828B7D\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">\u622a\u5c4f2024-09-22 19.38.29</span><span class=\"informations\">1920\u00d71209 243 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<hr>\n<h2>\n<a name=\"thinking-critical-thinking-3\" class=\"anchor\" href=\"#thinking-critical-thinking-3\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/thinking.png?v=12\" title=\":thinking:\" class=\"emoji\" alt=\":thinking:\" loading=\"lazy\" width=\"20\" height=\"20\"> Critical Thinking</h2>\n<h4>\n<a name=\"sparkles-positive-aspects-4\" class=\"anchor\" href=\"#sparkles-positive-aspects-4\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/sparkles.png?v=12\" title=\":sparkles:\" class=\"emoji\" alt=\":sparkles:\" loading=\"lazy\" width=\"20\" height=\"20\"> Positive Aspects</h4>\n<ul>\n<li>Idea: The idea is very interesting. In the real scenario of creating anime, artists will typically first do \u5206\u955c, and then draw the keyframe. This paper allows the automatic generation of the frames between the keyframes drawn by artists, significantly ease their broaden.</li>\n<li>Project: very solid and very impressive results</li>\n</ul>\n<hr>\n<h2>\n<a name=\"fountain-creative-thinking-5\" class=\"anchor\" href=\"#fountain-creative-thinking-5\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/fountain.png?v=12\" title=\":fountain:\" class=\"emoji\" alt=\":fountain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Creative thinking:</h2>\n<p>\u6211\u89c9\u5f97\u8fd9\u6837\u4ece\u5b9e\u9645\u9700\u6c42\u51fa\u53d1\u7684paper\u5728\u8fd9\u4e2a\u89c6\u9891\u751f\u6210\u975e\u5e38\u540c\u8d28\u5316\u7684\u5e74\u4ee3\u975e\u5e38\u6709insight. \u73b0\u5b9e\u751f\u6d3b\u4e2d,\u827a\u672f\u5bb6\u521b\u4f5c\u624b\u7ed8\u52a8\u6f2b\u7684\u65f6\u5019\u90fd\u662f\u5148\u5206\u955c\u7136\u540e\u518d\u753b\u5173\u952e\u5e27,\u63a5\u7740\u4e00\u5e27\u4e00\u5e27\u753b\u51fa\u5173\u952e\u5e27\u4e4b\u95f4\u7684\u6bcf\u4e00\u5e27,\u8fd9\u65e0\u7591\u662f\u975e\u5e38\u7e41\u7410\u7684.\u800c\u5728video interpolation model\u7684\u5e2e\u52a9\u4e0b, \u827a\u672f\u5bb6\u4e0d\u9700\u8981\u5728\u624b\u7ed8\u6bcf\u4e00\u5e27,\u800c\u662f\u53ef\u4ee5\u5728\u753b\u51fa\u5173\u952e\u5e27\u4e4b\u540e\u7531ai\u81ea\u52a8\u8865\u5168,\u8fd9\u5373\u4f7f\u5f97\u5267\u60c5\u548c\u753b\u98ce\u53ef\u4ee5\u7b26\u5408artist\u7684\u60f3\u6cd5,\u540c\u65f6\u4e5f\u53ef\u4ee5\u6781\u5927\u63d0\u5347\u6548\u7387, \u662fhuman\u548cgenerative ai\u7ed3\u5408\u7684\u4e00\u4e2a\u975e\u5e38\u597d\u7684\u4f8b\u5b50. \u4f46\u662f\u8fd9\u7bc7paper\u76ee\u524d\u4ecd\u7136\u9700\u8981\u63d0\u4f9b\u5f69\u8272\u7684\u9996\u5c3e\u4e24\u5e27,\u800c\u5b9e\u9645\u9700\u6c42\u4e2dartist\u5f80\u5f80\u4f1a\u4ee5sketch\u7684\u5f62\u5f0f\u6765\u753b\u5173\u952e\u5e27,\u56e0\u6b64,\u7528\u8fc7\u53ef\u4ee5\u8bb2\u4e0a\u8272\u4e0e\u63d2\u5e27\u7ed3\u5408,\u4f1a\u662f\u66f4\u7b26\u5408\u5b9e\u9645\u9700\u6c42\u7684\u5de5\u4f5c.</p>",
        "update": "2024-9-22 11:53:43"
    },
    "31": {
        "name": "Shuchang Xu",
        "content": "<h1>\n<a name=\"paper-titleworldscribe-towards-context-aware-live-visual-descriptions-1\" class=\"anchor\" href=\"#paper-titleworldscribe-towards-context-aware-live-visual-descriptions-1\"></a>[Paper Title](WorldScribe: Towards Context-Aware Live Visual Descriptions)</h1>\n<ul>\n<li>Conference/Journal (Name-Year): UIST 24 Best Paper</li>\n<li>Authors/group: Anhong Guo</li>\n<li>Paper Link: <a href=\"https://doi.org/10.1145/3654777.3676375\">https://doi.org/10.1145/3654777.3676375</a>\n</li>\n</ul>\n<hr>\n<h2>\n<a name=\"spiral_notepad-summary-2\" class=\"anchor\" href=\"#spiral_notepad-summary-2\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/spiral_notepad.png?v=12\" title=\":spiral_notepad:\" class=\"emoji\" alt=\":spiral_notepad:\" loading=\"lazy\" width=\"20\" height=\"20\"> Summary</h2>\n<p>This paper addresses the long-standing challenge of providing rich and contextual descriptions to blind and low-vision (BLV) users in real-time settings. It examines key trade-offs in generating live visual descriptions, particularly between richness and latency, as well as critical user contexts: user intent (what the user is focused on), visual context (how rapidly the environment changes), and audio context (whether there is nearby spoken content).</p>\n<p>Building on these findings, the authors introduce WorldScribe, a system that utilizes multiple vision-language models to deliver adaptive descriptions based on the user\u2019s context. An evaluation with six BLV participants demonstrated the potential of automated live visual descriptions and highlighted areas for improvement, such as calibrating descriptions to match the user\u2019s perspective, clarifying whether the description pertains to the current or a previous scene, and capturing additional details using ultra-wide cameras. The paper also discusses metrics for assessing future systems and the necessary advancements in vision-language models.</p>\n<hr>\n<h2>\n<a name=\"thinking-critical-thinking-3\" class=\"anchor\" href=\"#thinking-critical-thinking-3\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/thinking.png?v=12\" title=\":thinking:\" class=\"emoji\" alt=\":thinking:\" loading=\"lazy\" width=\"20\" height=\"20\"> Critical Thinking</h2>\n<h4>\n<a name=\"sparkles-positive-aspects-4\" class=\"anchor\" href=\"#sparkles-positive-aspects-4\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/sparkles.png?v=12\" title=\":sparkles:\" class=\"emoji\" alt=\":sparkles:\" loading=\"lazy\" width=\"20\" height=\"20\"> Positive Aspects</h4>\n<ul>\n<li>Idea: Tackles a highly significant problem.</li>\n<li>Solution: The consideration of user context is well-conceived.</li>\n<li>Evaluation: Provides many valuable insights.</li>\n</ul>\n<h4>\n<a name=\"cloud_with_rain-negative-aspects-5\" class=\"anchor\" href=\"#cloud_with_rain-negative-aspects-5\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/cloud_with_rain.png?v=12\" title=\":cloud_with_rain:\" class=\"emoji\" alt=\":cloud_with_rain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Negative Aspects</h4>\n<ul>\n<li>The system does not account for the spatial layout of the real environment; descriptions rely solely on images.</li>\n<li>The user might become overwhelmed when too many objects are described.</li>\n<li>The sound context seems more like an \u201cadd-on\u201d and may not be as crucial.</li>\n</ul>\n<h4>\n<a name=\"question-questions-6\" class=\"anchor\" href=\"#question-questions-6\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/question.png?v=12\" title=\":question:\" class=\"emoji\" alt=\":question:\" loading=\"lazy\" width=\"20\" height=\"20\"> : Questions</h4>\n<ul>\n<li>Why focus on objects and visual attributes? Many user intents may be vague (e.g., \u201cI want to go to a specific place\u201d). \u2192 This concern is resolved in a following section.</li>\n</ul>\n<hr>\n<h2>\n<a name=\"fountain-creative-thinking-7\" class=\"anchor\" href=\"#fountain-creative-thinking-7\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/fountain.png?v=12\" title=\":fountain:\" class=\"emoji\" alt=\":fountain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Creative thinking:</h2>\n<ul>\n<li>Incorporating spatial layout to avoid repetitive descriptions of the same objects.</li>\n<li>Using long-term memory to prevent repetitive descriptions of identical objects.</li>\n<li>Enabling real-time conversations during live descriptions.</li>\n<li>Prioritizing the description of objects users are physically interacting with.</li>\n<li>Employing ultra-wide cameras to capture more comprehensive information.</li>\n<li>Notifying users whether the description pertains to the current or a previous scene.</li>\n<li>Contextual Responsiveness: Providing just-in-time feedback during navigation and delivering longer descriptions for artistic works.</li>\n</ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"//bbs.hkustvis.org/uploads/default/original/2X/5/5361a28c588969efb9b128e7b440ba5d730b2cce.png\" data-download-href=\"//bbs.hkustvis.org/uploads/default/5361a28c588969efb9b128e7b440ba5d730b2cce\" title=\"Screenshot 2024-09-22 at 16.03.13\"><img src=\"//bbs.hkustvis.org/uploads/default/optimized/2X/5/5361a28c588969efb9b128e7b440ba5d730b2cce_2_690x465.png\" alt=\"Screenshot 2024-09-22 at 16.03.13\" data-base62-sha1=\"bTCOldxEgoCPw6BVjEfdJ5DTfbg\" width=\"690\" height=\"465\" srcset=\"//bbs.hkustvis.org/uploads/default/optimized/2X/5/5361a28c588969efb9b128e7b440ba5d730b2cce_2_690x465.png, //bbs.hkustvis.org/uploads/default/optimized/2X/5/5361a28c588969efb9b128e7b440ba5d730b2cce_2_1035x697.png 1.5x, //bbs.hkustvis.org/uploads/default/optimized/2X/5/5361a28c588969efb9b128e7b440ba5d730b2cce_2_1380x930.png 2x\" data-dominant-color=\"E7E7E7\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2024-09-22 at 16.03.13</span><span class=\"informations\">1976\u00d71334 495 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>",
        "update": "2024-9-22 8:3:44"
    },
    "34": {
        "name": "Yanjia Li",
        "content": "<h1>\n<a name=\"predicting-purchase-intent-deciphering-customer-interactions-with-ai-assistants-1\" class=\"anchor\" href=\"#predicting-purchase-intent-deciphering-customer-interactions-with-ai-assistants-1\"></a><a>Predicting Purchase Intent: Deciphering Customer Interactions with AI Assistants</a>\n</h1>\n<ul>\n<li>Conference/Journal (Name-Year): JMR under review</li>\n</ul>\n<hr>\n<h2>\n<a name=\"spiral_notepad-summary-2\" class=\"anchor\" href=\"#spiral_notepad-summary-2\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/spiral_notepad.png?v=12\" title=\":spiral_notepad:\" class=\"emoji\" alt=\":spiral_notepad:\" loading=\"lazy\" width=\"20\" height=\"20\"> Summary</h2>\n<p>This is a JMR paper l reviewed this week. The topic is related to language processing, conversation with AI assistant, and purchase prediction.</p>\n<p>In this paper, the author extracts verbs and nouns from conversations with an AI assistant to understand user purchase intentions. Word embeddings are generated for each verb and noun using a random walk, and the distance to the embeddings of key words (e.g., buy, order, purchase) is used to measure purchase intention.</p>\n<hr>\n<h2>\n<a name=\"thinking-critical-thinking-3\" class=\"anchor\" href=\"#thinking-critical-thinking-3\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/thinking.png?v=12\" title=\":thinking:\" class=\"emoji\" alt=\":thinking:\" loading=\"lazy\" width=\"20\" height=\"20\"> Critical Thinking</h2>\n<h4>\n<a name=\"sparkles-positive-aspects-4\" class=\"anchor\" href=\"#sparkles-positive-aspects-4\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/sparkles.png?v=12\" title=\":sparkles:\" class=\"emoji\" alt=\":sparkles:\" loading=\"lazy\" width=\"20\" height=\"20\"> Positive Aspects</h4>\n<ul>\n<li>Idea. It is an interesting idea to explore the conversation with AI assistant and use those information to understand the user intention. It is an area under exploration.</li>\n<li>Scalability. The method is simple and can be applied to large scale data.</li>\n<li>Evaluation. The evaluation is convincing. The results that some food keywords are closer to purchase behavior than the others are interesting.</li>\n</ul>\n<h4>\n<a name=\"cloud_with_rain-negative-aspects-5\" class=\"anchor\" href=\"#cloud_with_rain-negative-aspects-5\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/cloud_with_rain.png?v=12\" title=\":cloud_with_rain:\" class=\"emoji\" alt=\":cloud_with_rain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Negative Aspects</h4>\n<p>I think the major weakness of this paper are:</p>\n<ol>\n<li>\n<strong>Single type information.</strong> The author only make use of the text data for prediction. However, integrating heterogeneous data, such as time and location, could improve prediction accuracy. Some studies also use knowledge graphs to extract relationships between topics in historical sessions to model the evolution of user intention.</li>\n<li>\n<strong>Lack of Text Context:</strong> The paper focuses only on verbs and nouns, ignoring other contextual elements, which could lead to a loss of information. For example, a more common practice is to use the entire text messages are encoded using word2vec to summarize main topics and later connect them to high-level intentions. This approach provides clearer transitions from text details to high-level intentions, making results more explainable.</li>\n<li>\n<strong>No consideration of the cross-session connection.</strong> This paper treats each extracted word independently. It does not consider how to conversation topic changes over time, which may infer the evolvement of user intention. It does not consider the relation between conversation topic in consecutive session, thus cannot infer the further development of user intention.</li>\n</ol>\n<hr>\n<h2>\n<a name=\"fountain-creative-thinking-6\" class=\"anchor\" href=\"#fountain-creative-thinking-6\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/fountain.png?v=12\" title=\":fountain:\" class=\"emoji\" alt=\":fountain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Creative thinking:</h2>\n<p>Besides the text and purchase intention, l am also curious what else can we get by exploring the conversational data with AI assistant.</p>\n<p>As shown by many paper, people talk to AI assistant with more detailed and human-like expression. People talk to the AI robot with more honesty. It is a good gate to understand what people really think and who they are. The results could be much different from those obtained by survey.</p>\n<p>l am more interested to build an overall user protrait based on their conversational data. What are their communication style, what do they care, what troubles them in real life that they need help from AI. Those information can help make customized service for people in the future, rather than simply guessing whether the user would place an order soon or not.</p>",
        "update": "2024-9-22 13:56:50"
    },
    "35": {
        "name": "\u5e08\u5b87\u54f2 SHI Yu-Zhe ",
        "content": "<h2>\n<a name=\"broader-impact-reading-note-2438-1\" class=\"anchor\" href=\"#broader-impact-reading-note-2438-1\"></a>[Broader impact] Reading note <span class=\"hashtag\">#2438</span>\n</h2>\n<p><strong>Overall, the lab members are recommended to read this paper for acquiring an academic taste on the <em>potential broader impact</em> of digital twin techniques in the era of Large Language Model (LLM).</strong></p>\n<h2>\n<a name=\"newspaper_roll-paper-title-2\" class=\"anchor\" href=\"#newspaper_roll-paper-title-2\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/newspaper_roll.png?v=12\" title=\":newspaper_roll:\" class=\"emoji\" alt=\":newspaper_roll:\" loading=\"lazy\" width=\"20\" height=\"20\"> Paper title</h2>\n<p>Skilful nowcasting of extreme precipitation with NowcastNet</p>\n<ul>\n<li>Conference/Journal (Name-Year): Nature, 2023</li>\n<li>Author/group: Yuchen Zhang, Mingsheng Long, Kaiyuan Chen, Lanxiang Xing, Ronghua Jin, Michael I. Jordan, Jianmin Wang</li>\n<li>Paper link: <a href=\"https://doi.org/10.1038/s41586-023-06184-4\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Skilful nowcasting of extreme precipitation with NowcastNet | Nature</a>\n</li>\n</ul>\n<hr>\n<h2>\n<a name=\"writing_hand-summary-3\" class=\"anchor\" href=\"#writing_hand-summary-3\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/writing_hand.png?v=12\" title=\":writing_hand:\" class=\"emoji\" alt=\":writing_hand:\" loading=\"lazy\" width=\"20\" height=\"20\"> Summary</h2>\n<p>In this paper, the authors present NowcastNet, a novel deep learning model designed for the nowcasting of extreme precipitation events. The model integrates physical-evolution schemes with conditional learning methods within a neural network framework, optimizing end-to-end forecast error. NowcastNet was evaluated using radar observations from the USA and China, demonstrating superior performance in producing high-resolution, physically plausible nowcasts with lead times of up to 3 hours. The model was favorably ranked by professional meteorologists in a systematic evaluation, highlighting its potential for improving risk prevention and crisis management in the context of extreme precipitation events.</p>\n<hr>\n<h2>\n<a name=\"thinking-critical-thinking-4\" class=\"anchor\" href=\"#thinking-critical-thinking-4\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/thinking.png?v=12\" title=\":thinking:\" class=\"emoji\" alt=\":thinking:\" loading=\"lazy\" width=\"20\" height=\"20\"> Critical thinking</h2>\n<h4>\n<a name=\"laughing-positive-5\" class=\"anchor\" href=\"#laughing-positive-5\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/laughing.png?v=12\" title=\":laughing:\" class=\"emoji\" alt=\":laughing:\" loading=\"lazy\" width=\"20\" height=\"20\"> Positive</h4>\n<ul>\n<li>\n<strong>Idea</strong>: The integration of physical principles with deep learning for nowcasting represents a significant innovation in the field of meteorology.</li>\n<li>\n<strong>Project</strong>: NowcastNet\u2019s ability to generate detailed nowcasts of extreme precipitation events is a substantial advancement, particularly for high-stakes weather phenomena.</li>\n<li>\n<strong>Scalability</strong>: The model\u2019s architecture suggests potential for scalability, with possibilities to integrate additional meteorological data and physical laws for enhanced forecasting capabilities.</li>\n<li>\n<strong>Evaluation</strong>: The comprehensive evaluation against leading methods, including a systematic assessment by professional meteorologists, robustly validates NowcastNet\u2019s efficacy.</li>\n</ul>\n<h4>\n<a name=\"sob-negative-6\" class=\"anchor\" href=\"#sob-negative-6\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/sob.png?v=12\" title=\":sob:\" class=\"emoji\" alt=\":sob:\" loading=\"lazy\" width=\"20\" height=\"20\"> Negative</h4>\n<ul>\n<li>\n<strong>System</strong>: While NowcastNet shows promise, its reliance on high-quality radar data may limit its applicability in regions with less sophisticated weather monitoring infrastructure.</li>\n<li>\n<strong>Design</strong>: The complexity of the model could pose challenges for real-time operational deployment, where computational efficiency is critical.</li>\n<li>\n<strong>Dataset</strong>: The model\u2019s performance is contingent on the quality and representativeness of the training data, which may not fully encapsulate the diversity of global weather patterns.</li>\n</ul>\n<hr>\n<h2>\n<a name=\"creative-thinking-7\" class=\"anchor\" href=\"#creative-thinking-7\"></a>Creative thinking</h2>\n<p>Future research could explore the adaptation of NowcastNet for other meteorological phenomena beyond precipitation nowcasting, such as temperature forecasting or wind speed prediction. Additionally, the integration of satellite data and other atmospheric measurements could enhance the model\u2019s predictive accuracy and applicability in diverse geographic locations. Investigating the model\u2019s transferability to other languages and its potential for hybrid physical-AI models could also be promising avenues for extending the impact of this research.</p>\n<hr>\n<h2>\n<a name=\"rationale-of-recommendation-8\" class=\"anchor\" href=\"#rationale-of-recommendation-8\"></a>Rationale of recommendation</h2>\n<p>I recommend this paper to the Digital Twin community due to its innovative integration of physical laws and deep learning methodologies, which is highly relevant to the broader applications of digital twin technology. This study exemplifies how the principles of physics can be seamlessly merged with advanced neural networks to create a model that not only predicts but also understands the complex dynamics of weather systems. In the era of LLMs, where the emphasis is on understanding and generating human-like text, the techniques showcased in NowcastNet offer a compelling parallel. They highlight the potential for digital twins to go beyond static representations, becoming dynamic, predictive tools that can simulate and forecast real-world events with high accuracy.</p>",
        "update": "2024-9-21 13:20:5"
    },
    "36": {
        "name": "yujiahe",
        "content": "<h1>\n<a name=\"compositingvis-exploring-interactions-for-creating-composite-visualizations-in-immersive-environmentshttpsieeexploreieeeorgabstractdocument10669767casa_token2cklhec0o8saaaaazs4sioaxs9acbr6lbdj32v10rgxzs_dxawzkyh0i81mjrtkyofxmsotdoq1xhl2m2mf5efpcwoi-1\" class=\"anchor\" href=\"#compositingvis-exploring-interactions-for-creating-composite-visualizations-in-immersive-environmentshttpsieeexploreieeeorgabstractdocument10669767casa_token2cklhec0o8saaaaazs4sioaxs9acbr6lbdj32v10rgxzs_dxawzkyh0i81mjrtkyofxmsotdoq1xhl2m2mf5efpcwoi-1\"></a><a href=\"https://ieeexplore.ieee.org/abstract/document/10669767?casa_token=2CkLHeC0O8sAAAAA:zS4siOaxS9aCbR6LBDJ32V10rgxzS_DxAWZkyH0I81mjrtKYOfXmSOtDoQ1xhL2M2MF5efpcwOI\" rel=\"noopener nofollow ugc\">CompositingVis: Exploring Interactions for Creating Composite Visualizations in Immersive Environments</a>\n</h1>\n<ul>\n<li>Conference/Journal (Name-Year): TVCG 2024</li>\n<li>Authors/group: Qian Zhu</li>\n</ul>\n<hr>\n<h2>\n<a name=\"spiral_notepad-summary-2\" class=\"anchor\" href=\"#spiral_notepad-summary-2\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/spiral_notepad.png?v=12\" title=\":spiral_notepad:\" class=\"emoji\" alt=\":spiral_notepad:\" loading=\"lazy\" width=\"20\" height=\"20\"> Summary</h2>\n<p>The paper discusses a method for creating composite visualizations in immersive environments through embodied interactions, which is flexible and fluid and can facilitate understanding of the relationship between visualization views. It mainly focuses on creating interactions for compositing visualization, and it wants these interactions to be as natural and as concise as possible. For me, this paper looks like sort of applying a new technology ( immersive authoring) to an old problem ( compositing visualization ), which definitely offers insights but the purpose seems obvious as well.</p>\n<hr>\n<h2>\n<a name=\"thinking-critical-thinking-3\" class=\"anchor\" href=\"#thinking-critical-thinking-3\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/thinking.png?v=12\" title=\":thinking:\" class=\"emoji\" alt=\":thinking:\" loading=\"lazy\" width=\"20\" height=\"20\"> Critical Thinking</h2>\n<h4>\n<a name=\"sparkles-positive-aspects-4\" class=\"anchor\" href=\"#sparkles-positive-aspects-4\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/sparkles.png?v=12\" title=\":sparkles:\" class=\"emoji\" alt=\":sparkles:\" loading=\"lazy\" width=\"20\" height=\"20\"> Positive Aspects</h4>\n<ul>\n<li>Idea: The idea, as I summarized above, is pretty standard but I have to say it is formulated very well. I could follow the descriptions even without looking carefully at what it says. I put it in the positive aspect because it is positive for publishing. It demonstrates the very standard way of conducting research in the current day.</li>\n<li>Project: The project looks awesome. The graph they demonstrate and the design space they draw all look well.</li>\n<li>Evaluation: The evaluation is also pretty standard, recruiting participants and conducting experiments. Good work.</li>\n<li>The way the decompose the process of manipulating compositing visualization is worth studying. We could borrow such a method when designing interactions in other domains.</li>\n</ul>\n<h4>\n<a name=\"cloud_with_rain-negative-aspects-5\" class=\"anchor\" href=\"#cloud_with_rain-negative-aspects-5\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/cloud_with_rain.png?v=12\" title=\":cloud_with_rain:\" class=\"emoji\" alt=\":cloud_with_rain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Negative Aspects</h4>\n<ul>\n<li>The experience I imagine from the description is not natural for me in the first place ( still looks like an instruction manual). I would regard this whole design space as very teaching. I feel like they want to teach me something but most people including me don\u2019t want to learn when seeing new things. We want the feeling of transition and the feeling that I am born for interacting with this software, that kind of interaction feedback. Nevertheless, if the software is professional enough, the interaction design doesn\u2019t need to be natural. They just need to be smooth, as what was described by fluid interaction. Each interaction movement could be smooth to follow based on the previous one. Like the allocation of letters on the keyboard. They are not natural. We have a very long learning experience to go through. However, once we master this skill, we will have a very smooth and natural interaction experience, like we were born to interact with such a device while this is not the case lol.</li>\n<li>The teaching ( intro, user guidance, tutorial ) part is significantly important in game development. Many games are appraised because they have demonstrated their impressive teaching skill. Therefore, I think the training part in the experiment description needs to be more specific, instead of just a few sentences like \u201cWe let the participants enter a VR scene and briefly introduced the interactions in Fig. 3. We presented the design space to guide them to familiarize themselves with all the basic interaction operations through embodied experiences with pre-created cubes (e.g., scaling a cube or colliding two cubes). This served as foundational training for their subsequent experience with the pre-defined cases.\u201d It makes me feel that they are showing off that their interaction method is very natural but due to the interaction method being deeply connected with compositing visualizations most of their participants are not familiar with this topic, so it doesn\u2019t sound practical to me.</li>\n</ul>\n<hr>\n<h2>\n<a name=\"fountain-creative-thinking-6\" class=\"anchor\" href=\"#fountain-creative-thinking-6\"></a><img src=\"//bbs.hkustvis.org/images/emoji/twitter/fountain.png?v=12\" title=\":fountain:\" class=\"emoji\" alt=\":fountain:\" loading=\"lazy\" width=\"20\" height=\"20\"> Creative thinking:</h2>\n<ul>\n<li>Designing interaction is also domain-specific. Maybe we could work on this.</li>\n</ul>",
        "update": "2024-9-22 14:5:38"
    }
}